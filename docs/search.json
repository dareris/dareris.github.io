[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "dareris.github.io",
    "section": "",
    "text": "Classification\n\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nNov 19, 2023\n\n\nRishav Khatiwada\n\n\n\n\n\n\n  \n\n\n\n\nPost With Code\n\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nNov 19, 2023\n\n\nHarlow Malloc\n\n\n\n\n\n\n  \n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nNov 16, 2023\n\n\nTristan O’Malley\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/Classification/index.html",
    "href": "posts/Classification/index.html",
    "title": "Classification",
    "section": "",
    "text": "First we start with importing some of the required libraries\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as pl\nimport seaborn as sns\nfrom sklearn.feature_selection import SelectKBest, chi2\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import roc_curve, roc_auc_score, precision_recall_curve\nfrom scipy.stats import ttest_ind\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom scipy.stats import randint\n\nImporting the data set\n\ndf = pd.read_csv(\"Blacksburg_weather_dataset.csv\", index_col=\"DATE\")\ndf.head()\n\n\n\n\n\n\n\n\nSTATION\nLATITUDE\nLONGITUDE\nELEVATION\nDAPR\nMDPR\nPRCP\nSNOW\nSNWD\nTMAX\nTMIN\nTOBS\n\n\nDATE\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1/1/2000\nUSC00440766\n37.2039\n-80.4144\n641.0\nNaN\nNaN\n0.00\n0.0\n0.0\n56.0\n25.0\n33.0\n\n\n1/2/2000\nUSC00440766\n37.2039\n-80.4144\n641.0\nNaN\nNaN\n0.00\n0.0\n0.0\n59.0\n33.0\n36.0\n\n\n1/3/2000\nUSC00440766\n37.2039\n-80.4144\n641.0\nNaN\nNaN\n0.00\n0.0\n0.0\n63.0\n34.0\n41.0\n\n\n1/4/2000\nUSC00440766\n37.2039\n-80.4144\n641.0\nNaN\nNaN\n0.00\n0.0\n0.0\n66.0\n38.0\n58.0\n\n\n1/5/2000\nUSC00440766\n37.2039\n-80.4144\n641.0\nNaN\nNaN\n0.03\n0.0\n0.0\n60.0\n26.0\n26.0\n\n\n\n\n\n\n\nTo make the data machine learning ready, we first identify the missing values, for that\n\ndf.apply(pd.isnull).sum()/df.shape[0]\n\nSTATION      0.765982\nLATITUDE     0.765982\nLONGITUDE    0.765982\nELEVATION    0.765982\nDAPR         1.000000\nMDPR         1.000000\nPRCP         0.765982\nSNOW         0.769959\nSNWD         0.769959\nTMAX         0.766145\nTMIN         0.766253\nTOBS         0.766145\ndtype: float64\n\n\nLet’s only take some of the features that we may need and let’s convert them into columns\n\nbb_weather= df[[\"PRCP\", \"SNOW\", \"TMAX\", \"TMIN\"]].copy()\n\nbb_weather.columns = [\"rain\", \"snow\", \"max_T\", \"min_T\"]\n\nbb_weather.head()\n\n\n\n\n\n\n\n\nrain\nsnow\nmax_T\nmin_T\n\n\nDATE\n\n\n\n\n\n\n\n\n1/1/2000\n0.00\n0.0\n56.0\n25.0\n\n\n1/2/2000\n0.00\n0.0\n59.0\n33.0\n\n\n1/3/2000\n0.00\n0.0\n63.0\n34.0\n\n\n1/4/2000\n0.00\n0.0\n66.0\n38.0\n\n\n1/5/2000\n0.03\n0.0\n60.0\n26.0\n\n\n\n\n\n\n\n\nbb_weather.apply(pd.isnull).sum()/bb_weather.shape[0]\n\nrain     0.765982\nsnow     0.769959\nmax_T    0.766145\nmin_T    0.766253\ndtype: float64\n\n\nLet’s find out how many days there were rain in Blacksburg in the data set\n\nbb_weather[\"rain\"].value_counts()\n\nrain\n0.00    5408\n0.01     327\n0.02     214\n0.03     171\n0.05     129\n        ... \n2.31       1\n3.00       1\n1.37       1\n1.89       1\n4.42       1\nName: count, Length: 206, dtype: int64\n\n\nSince we are only concerned about the rainfall in Blacksburg and not the snowfall, let’s delete snowfall column\n\ndel bb_weather['snow']\n\nNow, let’s fill all the days which has rainfall values missing as 0. Here, we can also delete the concerned rows but here we are using 0 as the replacement.\n\nbb_weather[\"rain\"] = bb_weather[\"rain\"].fillna(0)\n\n\nbb_weather.apply(pd.isnull).sum()\n\nrain         0\nmax_T    28319\nmin_T    28323\ndtype: int64\n\n\nSince the weather of following day is mostly similar to the previous day we use forward fill in this case unlike rainfall\n\nbb_weather = bb_weather.fillna(method=\"ffill\")\n\ndocumentation of this file says if any item has 9999 then this is the missing values, so to identify them we have this\n\nbb_weather.apply(lambda x: (x == 9999).sum())\n\nrain     0\nmax_T    0\nmin_T    0\ndtype: int64\n\n\n\nbb_weather.index = pd.to_datetime(bb_weather.index)\n\nlet’s visualize the rainfall in different years.\n\npl.plot(bb_weather['rain'],color='blue',marker='o')\npl.xlabel('Year')\npl.ylabel('Rainfall(cm)')\n\nText(0, 0.5, 'Rainfall(cm)')\n\n\n\n\n\nNow, let’s create a new weather column based on the condition as rainy and non-rainy days.\n\nbb_weather['weather'] = 'not_rainy'\n\nbb_weather.loc[bb_weather['rain'] &gt; 0, 'weather'] = 'rainy'\n\nprint(bb_weather)\n\n            rain  max_T  min_T    weather\nDATE                                     \n2000-01-01  0.00   56.0   25.0  not_rainy\n2000-01-02  0.00   59.0   33.0  not_rainy\n2000-01-03  0.00   63.0   34.0  not_rainy\n2000-01-04  0.00   66.0   38.0  not_rainy\n2000-01-05  0.03   60.0   26.0      rainy\n...          ...    ...    ...        ...\nNaT         0.00   79.0   50.0  not_rainy\nNaT         0.00   79.0   50.0  not_rainy\nNaT         0.00   79.0   50.0  not_rainy\nNaT         0.00   79.0   50.0  not_rainy\nNaT         0.00   79.0   50.0  not_rainy\n\n[36963 rows x 4 columns]\n\n\nSince, it is difficult to deal with the words like rainy and non-rainy to perform our classification, we use one-hot encoding method to convert them into binary numbers as 1 and 0, where 1 represents rainy days.\n\nweather_encoded = pd.get_dummies(bb_weather['weather'], prefix='weather')\n\nbb_weather_encoded = pd.concat([bb_weather, weather_encoded], axis=1)\n\n\nbb_weather_encoded.drop('weather', axis=1, inplace=True)\n\nprint(bb_weather_encoded)\n\n            rain  max_T  min_T  weather_not_rainy  weather_rainy\nDATE                                                            \n2000-01-01  0.00   56.0   25.0               True          False\n2000-01-02  0.00   59.0   33.0               True          False\n2000-01-03  0.00   63.0   34.0               True          False\n2000-01-04  0.00   66.0   38.0               True          False\n2000-01-05  0.03   60.0   26.0              False           True\n...          ...    ...    ...                ...            ...\nNaT         0.00   79.0   50.0               True          False\nNaT         0.00   79.0   50.0               True          False\nNaT         0.00   79.0   50.0               True          False\nNaT         0.00   79.0   50.0               True          False\nNaT         0.00   79.0   50.0               True          False\n\n[36963 rows x 5 columns]\n\n\nSince we may get some of our values as negative also as the temperature might fall below 0 during winter, we convert our temperature in Farenheit to Kelvin using relation\n\\((T-32)*5/9 + 273.15\\)\n\nbb_weather_encoded['min_T(Kelvin)'] = (bb_weather_encoded['min_T'] - 32) * 5/9 + 273.15\nbb_weather_encoded['max_T(Kelvin)'] = (bb_weather_encoded['max_T'] - 32) * 5/9 + 273.15\n\n\nbb_weather_encoded.tail()\n\n\n\n\n\n\n\n\nrain\nmax_T\nmin_T\nweather_not_rainy\nweather_rainy\nmin_T(Kelvin)\nmax_T(Kelvin)\n\n\nDATE\n\n\n\n\n\n\n\n\n\n\n\nNaT\n0.0\n79.0\n50.0\nTrue\nFalse\n283.15\n299.261111\n\n\nNaT\n0.0\n79.0\n50.0\nTrue\nFalse\n283.15\n299.261111\n\n\nNaT\n0.0\n79.0\n50.0\nTrue\nFalse\n283.15\n299.261111\n\n\nNaT\n0.0\n79.0\n50.0\nTrue\nFalse\n283.15\n299.261111\n\n\nNaT\n0.0\n79.0\n50.0\nTrue\nFalse\n283.15\n299.261111\n\n\n\n\n\n\n\nFor simplified operation, we also need to convert the rain column into binary not only the weather column.\n\nrainfall = pd.DataFrame(bb_weather_encoded)\n\nrainfall['rain'] = (rainfall['rain'] &gt; 0).astype(int)\n\nTo make maximum temperature and the minimum temperatures as our main features, let’s use chi-squared test and select K best features.\n\nX = rainfall[['max_T(Kelvin)','min_T(Kelvin)']]\ny = rainfall['rain']\n\nk_best = SelectKBest(score_func=chi2, k=2)\n\nX_new = k_best.fit_transform(X, y)\n\nselected_feature_indices = k_best.get_support(indices=True)\n\nselected_features = X.columns[selected_feature_indices]\n\nprint(\"Selected features:\", selected_features)\n\nSelected features: Index(['max_T(Kelvin)', 'min_T(Kelvin)'], dtype='object')\n\n\nFor the classification of our data, we use the random forest classifier. By using this we find the classification report in the form of confusion matrix visualization.\nFor that let’s separate our data in terms of train and test part.\n\nstart_date_train = '2000-01-01'\nend_date_train = '2017-12-31'\nstart_date_test = '2018-01-01'\nend_date_test = '2023-10-05'\n\nwe then create the mask fo the data ranges and extract the data for train and test and finally apply the random forest classifier.\n\nmask_train = (X.index &gt;= start_date_train) & (X.index &lt;= end_date_train)\nmask_test = (X.index &gt;= start_date_test) & (X.index &lt;= end_date_test)\n\n\nX_train, y_train = X[mask_train], y[mask_train]\nX_test, y_test = X[mask_test], y[mask_test]\n\n\nrf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n\n\nrf_classifier.fit(X_train, y_train)\n\n\ny_pred = rf_classifier.predict(X_test)\n\nNext is finding the classification report and the visualization in confusion matrix.\n\nreport = classification_report(y_test, y_pred)\nprint('Classification Report:\\n', report)\n\n\nconfusion = confusion_matrix(y_test, y_pred)\nprint('Confusion Matrix:\\n', confusion)\n\n\npl.figure(figsize=(8, 6))\nsns.heatmap(confusion, annot=True, fmt='d', cmap='Oranges', xticklabels=np.unique(y), yticklabels=np.unique(y))\npl.xlabel('Predicted')\npl.ylabel('Actual')\npl.title('Confusion Matrix')\npl.show()\n\nClassification Report:\n               precision    recall  f1-score   support\n\n           0       0.70      0.78      0.74      1299\n           1       0.57      0.47      0.52       807\n\n    accuracy                           0.66      2106\n   macro avg       0.64      0.63      0.63      2106\nweighted avg       0.65      0.66      0.66      2106\n\nConfusion Matrix:\n [[1017  282]\n [ 428  379]]\n\n\n\n\n\nNow to see if we can improve our results we perform feature importance.\n\nfeature_importance = rf_classifier.feature_importances_\n\n\nfeature_names = ['max_T(Kelvin)','min_T(Kelvin)']\nimportance_df = pd.DataFrame({'Feature': feature_names, 'Importance': feature_importance})\n\n\nimportance_df = importance_df.sort_values(by='Importance', ascending=False)\n\n\npl.figure(figsize=(10, 6))\npl.bar(importance_df['Feature'], importance_df['Importance'])\npl.xticks(rotation=45)\npl.title('Feature Importance for Snowfall Classification')\npl.show()\n\n\n\n\nThen we define the hyper parameters and their possible values:\nHere, we define the no. of trees in the forest(random forest classifier forest not the forest in nature), its max depth and we define the minimum samples to split internal node.\n\nparam_grid = {\n    'n_estimators': [100, 200, 300],\n    \n    'max_depth': [None, 10, 20],\n    \n    'min_samples_split': [2, 5, 10],\n    \n    'min_samples_leaf': [1, 2, 4],\n    }\n\n\nrandom_search = RandomizedSearchCV(rf_classifier, param_grid, n_iter=5, cv=5, scoring='accuracy', random_state=42)\n\n\nrandom_search.fit(X, y)\n\n\nbest_params = random_search.best_params_\nbest_model = random_search.best_estimator_\n\nprint(\"Best Hyperparameters:\")\nprint(best_params)\n\n\nresults = pd.DataFrame(random_search.cv_results_)\n\nBest Hyperparameters:\n{'n_estimators': 100, 'min_samples_split': 2, 'min_samples_leaf': 4, 'max_depth': None}\n\n\nAnd for the pivot table and confusion matrix visualization\n\npivot_table = results.pivot_table(index=['param_n_estimators', 'param_max_depth'],\n                                  columns='param_min_samples_split',\n                                  values='mean_test_score')\n\n\npl.figure(figsize=(10, 6))\nsns.heatmap(pivot_table, annot=True, fmt=\".3f\", cmap=\"YlGnBu\")\npl.title(\"Grid Search Results for Hyperparameters\")\npl.xlabel(\"min_samples_split\")\npl.ylabel(\"n_estimators / max_depth\")\npl.show()\n\n\n\n\nTo identify, how accurate is the result that we obtained\n\nfinal_model = RandomForestClassifier(\n    n_estimators=100,\n    max_depth=10,\n    min_samples_split=10,\n    min_samples_leaf=2,\n    random_state=42\n)\n\n\nfinal_model.fit(X, y)\n\ny_pred = final_model.predict(X)\n\n\nfinal_accuracy = accuracy_score(y, y_pred)\n\nprint(f\"Final Model Accuracy: {final_accuracy:.2f}\")\n\nFinal Model Accuracy: 0.94\n\n\nAnd hence following the same steps that we did earlier, we get new classification results.\n\nreport1 = classification_report(y, y_pred)\nprint('Classification Report:\\n', report1)\n\nClassification Report:\n               precision    recall  f1-score   support\n\n           0       0.95      0.98      0.97     33721\n           1       0.72      0.50      0.59      3242\n\n    accuracy                           0.94     36963\n   macro avg       0.84      0.74      0.78     36963\nweighted avg       0.93      0.94      0.93     36963\n\n\n\n\nconfusion = confusion_matrix(y, y_pred)\n\n# Create a heatmap of the confusion matrix\npl.figure(figsize=(8, 6))\nsns.heatmap(confusion, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False, xticklabels=True, yticklabels=True)\npl.xlabel('Predicted')\npl.ylabel('Actual')\npl.title('Confusion Matrix')\npl.show()\n\n\n\n\nNow to examine the Random Forest Model’s performance, we first make ROC and Precision-Recall curves.\nIt also help us identify the trade offs involved in different threshold settings.\n\ny_prob = best_model.predict_proba(X_test)[:, 1]\n\nfpr, tpr, thresholds_roc = roc_curve(y_test, y_prob)\n\nauc_roc = roc_auc_score(y_test, y_prob)\n\nprecision, recall, thresholds_pr = precision_recall_curve(y_test, y_prob)\n\nAnd to visualize:\n\npl.figure(figsize=(12, 6))\n\n# Plotting ROC curve:\npl.subplot(1, 2, 1)\npl.plot(fpr, tpr, label=f'ROC Curve (AUC = {auc_roc:.2f})')\npl.plot([0, 1], [0, 1], 'k--', label='Random')\npl.xlabel('False Positive Rate')\npl.ylabel('True Positive Rate')\npl.title('ROC Curve')\npl.legend()\n\n# Plotting PR curve:\npl.subplot(1, 2, 2)\npl.plot(recall, precision, label='Precision-Recall Curve')\npl.xlabel('Recall')\npl.ylabel('Precision')\npl.title('Precision-Recall Curve')\npl.legend()\n\n# Adjusting the layout to prevent overlap of subplots\npl.tight_layout()\n\n\n\n\nAnd Finally for the T-test results and the accuracy scores:\n\nIn the future, if we want to know the accuracy of any dataframe directly we can use the following code to directly give the accuracy score once we upload a file which has been cleaned to make ML ready.\n\n\"\n\n# Accuracy score Prediction\n\n#Load the trained ML model\n\nmodel = joblib.load(\"filename\") \\# Replace with the path to your model file\n\ndef predict_snowfall(min_temp, max_temp): try:\n\n\\# Create a DataFrame with the input data\n\ninput_data = pd.DataFrame({'min_T(Kelvin)': \\[min_temp\\], 'max_T(Kelvin)': \\[max_temp\\]})\n# Make predictions\nprediction = model.predict(input_data)\n\nreturn prediction[0]\nexcept Exception as e: return str(e)\n\nif **name** == '**main**': print(\"Temperature to rainfall Prediction\")\n\nmin_temp = float(input(\"Enter Minimum Temperature (Kelvin):\"))\n\nmax_temp = float(input(\"Enter Maximum Temperature (Kelvin):\"))\nresult = predict_rainfall(min_temp, max_temp) print(f”rainfall Prediction: {result}“) ```\n”"
  }
]