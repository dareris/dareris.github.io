{
  "hash": "0b0ddbc21ccefbf328a68767ba4fa016",
  "result": {
    "markdown": "---\ntitle: \"Classification\"\nauthor: \"Rishav Khatiwada\"\ndate: \"2023-11-19\"\ncategories: [news, code, analysis]\nimage: \"image.jpg\"\n---\n\nFirst we start with importing some of the required libraries\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as pl\nimport seaborn as sns\nfrom sklearn.feature_selection import SelectKBest, chi2\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import roc_curve, roc_auc_score, precision_recall_curve\nfrom scipy.stats import ttest_ind\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom scipy.stats import randint\n```\n:::\n\n\nImporting the data set\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\ndf = pd.read_csv(\"Blacksburg_weather_dataset.csv\", index_col=\"DATE\")\ndf.head()\n```\n\n::: {.cell-output .cell-output-display execution_count=2}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>STATION</th>\n      <th>LATITUDE</th>\n      <th>LONGITUDE</th>\n      <th>ELEVATION</th>\n      <th>DAPR</th>\n      <th>MDPR</th>\n      <th>PRCP</th>\n      <th>SNOW</th>\n      <th>SNWD</th>\n      <th>TMAX</th>\n      <th>TMIN</th>\n      <th>TOBS</th>\n    </tr>\n    <tr>\n      <th>DATE</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1/1/2000</th>\n      <td>USC00440766</td>\n      <td>37.2039</td>\n      <td>-80.4144</td>\n      <td>641.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.00</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>56.0</td>\n      <td>25.0</td>\n      <td>33.0</td>\n    </tr>\n    <tr>\n      <th>1/2/2000</th>\n      <td>USC00440766</td>\n      <td>37.2039</td>\n      <td>-80.4144</td>\n      <td>641.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.00</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>59.0</td>\n      <td>33.0</td>\n      <td>36.0</td>\n    </tr>\n    <tr>\n      <th>1/3/2000</th>\n      <td>USC00440766</td>\n      <td>37.2039</td>\n      <td>-80.4144</td>\n      <td>641.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.00</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>63.0</td>\n      <td>34.0</td>\n      <td>41.0</td>\n    </tr>\n    <tr>\n      <th>1/4/2000</th>\n      <td>USC00440766</td>\n      <td>37.2039</td>\n      <td>-80.4144</td>\n      <td>641.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.00</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>66.0</td>\n      <td>38.0</td>\n      <td>58.0</td>\n    </tr>\n    <tr>\n      <th>1/5/2000</th>\n      <td>USC00440766</td>\n      <td>37.2039</td>\n      <td>-80.4144</td>\n      <td>641.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.03</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>60.0</td>\n      <td>26.0</td>\n      <td>26.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nTo make the data machine learning ready, we first identify the missing values, for that\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\ndf.apply(pd.isnull).sum()/df.shape[0]\n```\n\n::: {.cell-output .cell-output-display execution_count=3}\n```\nSTATION      0.765982\nLATITUDE     0.765982\nLONGITUDE    0.765982\nELEVATION    0.765982\nDAPR         1.000000\nMDPR         1.000000\nPRCP         0.765982\nSNOW         0.769959\nSNWD         0.769959\nTMAX         0.766145\nTMIN         0.766253\nTOBS         0.766145\ndtype: float64\n```\n:::\n:::\n\n\nLet's only take some of the features that we may need and let's convert them into columns\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\nbb_weather= df[[\"PRCP\", \"SNOW\", \"TMAX\", \"TMIN\"]].copy()\n\nbb_weather.columns = [\"rain\", \"snow\", \"max_T\", \"min_T\"]\n\nbb_weather.head()\n```\n\n::: {.cell-output .cell-output-display execution_count=4}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>rain</th>\n      <th>snow</th>\n      <th>max_T</th>\n      <th>min_T</th>\n    </tr>\n    <tr>\n      <th>DATE</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1/1/2000</th>\n      <td>0.00</td>\n      <td>0.0</td>\n      <td>56.0</td>\n      <td>25.0</td>\n    </tr>\n    <tr>\n      <th>1/2/2000</th>\n      <td>0.00</td>\n      <td>0.0</td>\n      <td>59.0</td>\n      <td>33.0</td>\n    </tr>\n    <tr>\n      <th>1/3/2000</th>\n      <td>0.00</td>\n      <td>0.0</td>\n      <td>63.0</td>\n      <td>34.0</td>\n    </tr>\n    <tr>\n      <th>1/4/2000</th>\n      <td>0.00</td>\n      <td>0.0</td>\n      <td>66.0</td>\n      <td>38.0</td>\n    </tr>\n    <tr>\n      <th>1/5/2000</th>\n      <td>0.03</td>\n      <td>0.0</td>\n      <td>60.0</td>\n      <td>26.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\nbb_weather.apply(pd.isnull).sum()/bb_weather.shape[0]\n```\n\n::: {.cell-output .cell-output-display execution_count=5}\n```\nrain     0.765982\nsnow     0.769959\nmax_T    0.766145\nmin_T    0.766253\ndtype: float64\n```\n:::\n:::\n\n\nLet's find out how many days there were rain in Blacksburg in the data set\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\nbb_weather[\"rain\"].value_counts()\n```\n\n::: {.cell-output .cell-output-display execution_count=6}\n```\nrain\n0.00    5408\n0.01     327\n0.02     214\n0.03     171\n0.05     129\n        ... \n2.31       1\n3.00       1\n1.37       1\n1.89       1\n4.42       1\nName: count, Length: 206, dtype: int64\n```\n:::\n:::\n\n\nSince we are only concerned about the rainfall in Blacksburg and not the snowfall, let's delete snowfall column\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\ndel bb_weather['snow']\n```\n:::\n\n\nNow, let's fill all the days which has rainfall values missing as 0. Here, we can also delete the concerned rows but here we are using 0 as the replacement.\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\nbb_weather[\"rain\"] = bb_weather[\"rain\"].fillna(0)\n```\n:::\n\n\n::: {.cell execution_count=9}\n``` {.python .cell-code}\nbb_weather.apply(pd.isnull).sum()\n```\n\n::: {.cell-output .cell-output-display execution_count=9}\n```\nrain         0\nmax_T    28319\nmin_T    28323\ndtype: int64\n```\n:::\n:::\n\n\nSince the weather of following day is mostly similar to the previous day we use forward fill in this case unlike rainfall\n\n::: {.cell execution_count=10}\n``` {.python .cell-code}\nbb_weather = bb_weather.fillna(method=\"ffill\")\n```\n:::\n\n\ndocumentation of this file says if any item has 9999 then this is the missing values, so to identify them we have this\n\n::: {.cell execution_count=11}\n``` {.python .cell-code}\nbb_weather.apply(lambda x: (x == 9999).sum())\n```\n\n::: {.cell-output .cell-output-display execution_count=11}\n```\nrain     0\nmax_T    0\nmin_T    0\ndtype: int64\n```\n:::\n:::\n\n\n::: {.cell execution_count=12}\n``` {.python .cell-code}\nbb_weather.index = pd.to_datetime(bb_weather.index)\n```\n:::\n\n\nlet's visualize the rainfall in different years.\n\n::: {.cell execution_count=13}\n``` {.python .cell-code}\npl.plot(bb_weather['rain'],color='blue',marker='o')\npl.xlabel('Year')\npl.ylabel('Rainfall(cm)')\n```\n\n::: {.cell-output .cell-output-display execution_count=13}\n```\nText(0, 0.5, 'Rainfall(cm)')\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-14-output-2.png){width=576 height=429}\n:::\n:::\n\n\nNow, let's create a new weather column based on the condition as rainy and non-rainy days.\n\n::: {.cell execution_count=14}\n``` {.python .cell-code}\nbb_weather['weather'] = 'not_rainy'\n\nbb_weather.loc[bb_weather['rain'] > 0, 'weather'] = 'rainy'\n\nprint(bb_weather)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n            rain  max_T  min_T    weather\nDATE                                     \n2000-01-01  0.00   56.0   25.0  not_rainy\n2000-01-02  0.00   59.0   33.0  not_rainy\n2000-01-03  0.00   63.0   34.0  not_rainy\n2000-01-04  0.00   66.0   38.0  not_rainy\n2000-01-05  0.03   60.0   26.0      rainy\n...          ...    ...    ...        ...\nNaT         0.00   79.0   50.0  not_rainy\nNaT         0.00   79.0   50.0  not_rainy\nNaT         0.00   79.0   50.0  not_rainy\nNaT         0.00   79.0   50.0  not_rainy\nNaT         0.00   79.0   50.0  not_rainy\n\n[36963 rows x 4 columns]\n```\n:::\n:::\n\n\nSince, it is difficult to deal with the words like rainy and non-rainy to perform our classification, we use one-hot encoding method to convert them into binary numbers as 1 and 0, where 1 represents rainy days.\n\n::: {.cell execution_count=15}\n``` {.python .cell-code}\nweather_encoded = pd.get_dummies(bb_weather['weather'], prefix='weather')\n\nbb_weather_encoded = pd.concat([bb_weather, weather_encoded], axis=1)\n\n\nbb_weather_encoded.drop('weather', axis=1, inplace=True)\n\nprint(bb_weather_encoded)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n            rain  max_T  min_T  weather_not_rainy  weather_rainy\nDATE                                                            \n2000-01-01  0.00   56.0   25.0               True          False\n2000-01-02  0.00   59.0   33.0               True          False\n2000-01-03  0.00   63.0   34.0               True          False\n2000-01-04  0.00   66.0   38.0               True          False\n2000-01-05  0.03   60.0   26.0              False           True\n...          ...    ...    ...                ...            ...\nNaT         0.00   79.0   50.0               True          False\nNaT         0.00   79.0   50.0               True          False\nNaT         0.00   79.0   50.0               True          False\nNaT         0.00   79.0   50.0               True          False\nNaT         0.00   79.0   50.0               True          False\n\n[36963 rows x 5 columns]\n```\n:::\n:::\n\n\nSince we may get some of our values as negative also as the temperature might fall below 0 during winter, we convert our temperature in Farenheit to Kelvin using relation\n\n$(T-32)*5/9 + 273.15$\n\n::: {.cell execution_count=16}\n``` {.python .cell-code}\nbb_weather_encoded['min_T(Kelvin)'] = (bb_weather_encoded['min_T'] - 32) * 5/9 + 273.15\nbb_weather_encoded['max_T(Kelvin)'] = (bb_weather_encoded['max_T'] - 32) * 5/9 + 273.15\n```\n:::\n\n\n::: {.cell execution_count=17}\n``` {.python .cell-code}\nbb_weather_encoded.tail()\n```\n\n::: {.cell-output .cell-output-display execution_count=17}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>rain</th>\n      <th>max_T</th>\n      <th>min_T</th>\n      <th>weather_not_rainy</th>\n      <th>weather_rainy</th>\n      <th>min_T(Kelvin)</th>\n      <th>max_T(Kelvin)</th>\n    </tr>\n    <tr>\n      <th>DATE</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>NaT</th>\n      <td>0.0</td>\n      <td>79.0</td>\n      <td>50.0</td>\n      <td>True</td>\n      <td>False</td>\n      <td>283.15</td>\n      <td>299.261111</td>\n    </tr>\n    <tr>\n      <th>NaT</th>\n      <td>0.0</td>\n      <td>79.0</td>\n      <td>50.0</td>\n      <td>True</td>\n      <td>False</td>\n      <td>283.15</td>\n      <td>299.261111</td>\n    </tr>\n    <tr>\n      <th>NaT</th>\n      <td>0.0</td>\n      <td>79.0</td>\n      <td>50.0</td>\n      <td>True</td>\n      <td>False</td>\n      <td>283.15</td>\n      <td>299.261111</td>\n    </tr>\n    <tr>\n      <th>NaT</th>\n      <td>0.0</td>\n      <td>79.0</td>\n      <td>50.0</td>\n      <td>True</td>\n      <td>False</td>\n      <td>283.15</td>\n      <td>299.261111</td>\n    </tr>\n    <tr>\n      <th>NaT</th>\n      <td>0.0</td>\n      <td>79.0</td>\n      <td>50.0</td>\n      <td>True</td>\n      <td>False</td>\n      <td>283.15</td>\n      <td>299.261111</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nFor simplified operation, we also need to convert the rain column into binary not only the weather column.\n\n::: {.cell execution_count=18}\n``` {.python .cell-code}\nrainfall = pd.DataFrame(bb_weather_encoded)\n\nrainfall['rain'] = (rainfall['rain'] > 0).astype(int)\n```\n:::\n\n\nTo make maximum temperature and the minimum temperatures as our main features, let's use chi-squared test and select K best features.\n\n::: {.cell execution_count=19}\n``` {.python .cell-code}\nX = rainfall[['max_T(Kelvin)','min_T(Kelvin)']]\ny = rainfall['rain']\n\nk_best = SelectKBest(score_func=chi2, k=2)\n\nX_new = k_best.fit_transform(X, y)\n\nselected_feature_indices = k_best.get_support(indices=True)\n\nselected_features = X.columns[selected_feature_indices]\n\nprint(\"Selected features:\", selected_features)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSelected features: Index(['max_T(Kelvin)', 'min_T(Kelvin)'], dtype='object')\n```\n:::\n:::\n\n\nFor the classification of our data, we use the random forest classifier. By using this we find the classification report in the form of confusion matrix visualization.\n\nFor that let's separate our data in terms of train and test part.\n\n::: {.cell execution_count=20}\n``` {.python .cell-code}\nstart_date_train = '2000-01-01'\nend_date_train = '2017-12-31'\nstart_date_test = '2018-01-01'\nend_date_test = '2023-10-05'\n```\n:::\n\n\nwe then create the mask fo the data ranges and extract the data for train and test and finally apply the random forest classifier.\n\n::: {.cell execution_count=21}\n``` {.python .cell-code}\nmask_train = (X.index >= start_date_train) & (X.index <= end_date_train)\nmask_test = (X.index >= start_date_test) & (X.index <= end_date_test)\n\n\nX_train, y_train = X[mask_train], y[mask_train]\nX_test, y_test = X[mask_test], y[mask_test]\n\n\nrf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n\n\nrf_classifier.fit(X_train, y_train)\n\n\ny_pred = rf_classifier.predict(X_test)\n```\n:::\n\n\nNext is finding the classification report and the visualization in confusion matrix.\n\n::: {.cell execution_count=22}\n``` {.python .cell-code}\nreport = classification_report(y_test, y_pred)\nprint('Classification Report:\\n', report)\n\n\nconfusion = confusion_matrix(y_test, y_pred)\nprint('Confusion Matrix:\\n', confusion)\n\n\npl.figure(figsize=(8, 6))\nsns.heatmap(confusion, annot=True, fmt='d', cmap='Oranges', xticklabels=np.unique(y), yticklabels=np.unique(y))\npl.xlabel('Predicted')\npl.ylabel('Actual')\npl.title('Confusion Matrix')\npl.show()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nClassification Report:\n               precision    recall  f1-score   support\n\n           0       0.70      0.78      0.74      1299\n           1       0.57      0.47      0.52       807\n\n    accuracy                           0.66      2106\n   macro avg       0.64      0.63      0.63      2106\nweighted avg       0.65      0.66      0.66      2106\n\nConfusion Matrix:\n [[1017  282]\n [ 428  379]]\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-23-output-2.png){width=631 height=523}\n:::\n:::\n\n\nNow to see if we can improve our results we perform feature importance.\n\n::: {.cell execution_count=23}\n``` {.python .cell-code}\nfeature_importance = rf_classifier.feature_importances_\n\n\nfeature_names = ['max_T(Kelvin)','min_T(Kelvin)']\nimportance_df = pd.DataFrame({'Feature': feature_names, 'Importance': feature_importance})\n\n\nimportance_df = importance_df.sort_values(by='Importance', ascending=False)\n\n\npl.figure(figsize=(10, 6))\npl.bar(importance_df['Feature'], importance_df['Importance'])\npl.xticks(rotation=45)\npl.title('Feature Importance for Snowfall Classification')\npl.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-24-output-1.png){width=794 height=568}\n:::\n:::\n\n\nThen we define the hyper parameters and their possible values:\n\nHere, we define the no. of trees in the forest(random forest classifier forest not the forest in nature), its max depth and we define the minimum samples to split internal node.\n\n::: {.cell execution_count=24}\n``` {.python .cell-code}\nparam_grid = {\n    'n_estimators': [100, 200, 300],\n    \n    'max_depth': [None, 10, 20],\n    \n    'min_samples_split': [2, 5, 10],\n    \n    'min_samples_leaf': [1, 2, 4],\n    }\n\n\nrandom_search = RandomizedSearchCV(rf_classifier, param_grid, n_iter=5, cv=5, scoring='accuracy', random_state=42)\n\n\nrandom_search.fit(X, y)\n\n\nbest_params = random_search.best_params_\nbest_model = random_search.best_estimator_\n\nprint(\"Best Hyperparameters:\")\nprint(best_params)\n\n\nresults = pd.DataFrame(random_search.cv_results_)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nBest Hyperparameters:\n{'n_estimators': 100, 'min_samples_split': 2, 'min_samples_leaf': 4, 'max_depth': None}\n```\n:::\n:::\n\n\nAnd for the pivot table and confusion matrix visualization\n\n::: {.cell execution_count=25}\n``` {.python .cell-code}\npivot_table = results.pivot_table(index=['param_n_estimators', 'param_max_depth'],\n                                  columns='param_min_samples_split',\n                                  values='mean_test_score')\n\n\npl.figure(figsize=(10, 6))\nsns.heatmap(pivot_table, annot=True, fmt=\".3f\", cmap=\"YlGnBu\")\npl.title(\"Grid Search Results for Hyperparameters\")\npl.xlabel(\"min_samples_split\")\npl.ylabel(\"n_estimators / max_depth\")\npl.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-26-output-1.png){width=741 height=523}\n:::\n:::\n\n\nTo identify, how accurate is the result that we obtained\n\n::: {.cell execution_count=26}\n``` {.python .cell-code}\nfinal_model = RandomForestClassifier(\n    n_estimators=100,\n    max_depth=10,\n    min_samples_split=10,\n    min_samples_leaf=2,\n    random_state=42\n)\n\n\nfinal_model.fit(X, y)\n\ny_pred = final_model.predict(X)\n\n\nfinal_accuracy = accuracy_score(y, y_pred)\n\nprint(f\"Final Model Accuracy: {final_accuracy:.2f}\")\n\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nFinal Model Accuracy: 0.94\n```\n:::\n:::\n\n\nAnd hence following the same steps that we did earlier, we get new classification results.\n\n::: {.cell execution_count=27}\n``` {.python .cell-code}\nreport1 = classification_report(y, y_pred)\nprint('Classification Report:\\n', report1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nClassification Report:\n               precision    recall  f1-score   support\n\n           0       0.95      0.98      0.97     33721\n           1       0.72      0.50      0.59      3242\n\n    accuracy                           0.94     36963\n   macro avg       0.84      0.74      0.78     36963\nweighted avg       0.93      0.94      0.93     36963\n\n```\n:::\n:::\n\n\n::: {.cell execution_count=28}\n``` {.python .cell-code}\nconfusion = confusion_matrix(y, y_pred)\n\n# Create a heatmap of the confusion matrix\npl.figure(figsize=(8, 6))\nsns.heatmap(confusion, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False, xticklabels=True, yticklabels=True)\npl.xlabel('Predicted')\npl.ylabel('Actual')\npl.title('Confusion Matrix')\npl.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-29-output-1.png){width=655 height=523}\n:::\n:::\n\n\nNow to examine the Random Forest Model's performance, we first make ROC and Precision-Recall curves.\n\nIt also help us identify the trade offs involved in different threshold settings.\n\n::: {.cell execution_count=29}\n``` {.python .cell-code}\ny_prob = best_model.predict_proba(X_test)[:, 1]\n\nfpr, tpr, thresholds_roc = roc_curve(y_test, y_prob)\n\nauc_roc = roc_auc_score(y_test, y_prob)\n\nprecision, recall, thresholds_pr = precision_recall_curve(y_test, y_prob)\n```\n:::\n\n\nAnd to visualize:\n\n::: {.cell execution_count=30}\n``` {.python .cell-code}\npl.figure(figsize=(12, 6))\n\n# Plotting ROC curve:\npl.subplot(1, 2, 1)\npl.plot(fpr, tpr, label=f'ROC Curve (AUC = {auc_roc:.2f})')\npl.plot([0, 1], [0, 1], 'k--', label='Random')\npl.xlabel('False Positive Rate')\npl.ylabel('True Positive Rate')\npl.title('ROC Curve')\npl.legend()\n\n# Plotting PR curve:\npl.subplot(1, 2, 2)\npl.plot(recall, precision, label='Precision-Recall Curve')\npl.xlabel('Recall')\npl.ylabel('Precision')\npl.title('Precision-Recall Curve')\npl.legend()\n\n# Adjusting the layout to prevent overlap of subplots\npl.tight_layout()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-31-output-1.png){width=1142 height=566}\n:::\n:::\n\n\nAnd Finally for the T-test results and the accuracy scores:\n\n```         \n\nIn the future, if we want to know the accuracy of any dataframe directly we can use the following code to directly give the accuracy score once we upload a file which has been cleaned to make ML ready.\n\n\"\n\n# Accuracy score Prediction\n\n#Load the trained ML model\n\nmodel = joblib.load(\"filename\") \\# Replace with the path to your model file\n\ndef predict_snowfall(min_temp, max_temp): try:\n\n\\# Create a DataFrame with the input data\n\ninput_data = pd.DataFrame({'min_T(Kelvin)': \\[min_temp\\], 'max_T(Kelvin)': \\[max_temp\\]})\n```\n\n```         \n# Make predictions\nprediction = model.predict(input_data)\n\nreturn prediction[0]\n```\n\nexcept Exception as e: return str(e)\n\n```         \n\nif **name** == '**main**': print(\"Temperature to rainfall Prediction\")\n\nmin_temp = float(input(\"Enter Minimum Temperature (Kelvin):\"))\n\nmax_temp = float(input(\"Enter Maximum Temperature (Kelvin):\"))\n```\n\nresult = predict_rainfall(min_temp, max_temp) print(f\"rainfall Prediction: {result}\") \\`\\`\\`\n\n\"\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}