[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "dareris.github.io",
    "section": "",
    "text": "Classification\n\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nNov 19, 2023\n\n\nRishav Khatiwada\n\n\n\n\n\n\n  \n\n\n\n\nPost With Code\n\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nNov 19, 2023\n\n\nHarlow Malloc\n\n\n\n\n\n\n  \n\n\n\n\nRegression\n\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nNov 19, 2023\n\n\nRishav Khatiwada\n\n\n\n\n\n\n  \n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nNov 16, 2023\n\n\nTristan O’Malley\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/Classification/index.html",
    "href": "posts/Classification/index.html",
    "title": "Classification",
    "section": "",
    "text": "First we start with importing some of the required libraries\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as pl\nimport seaborn as sns\nfrom sklearn.feature_selection import SelectKBest, chi2\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import roc_curve, roc_auc_score, precision_recall_curve\nfrom scipy.stats import ttest_ind\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom scipy.stats import randint\n\nImporting the data set\n\ndf = pd.read_csv(\"Blacksburg_weather_dataset.csv\", index_col=\"DATE\")\ndf.head()\n\n\n\n\n\n\n\n\nSTATION\nLATITUDE\nLONGITUDE\nELEVATION\nDAPR\nMDPR\nPRCP\nSNOW\nSNWD\nTMAX\nTMIN\nTOBS\n\n\nDATE\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1/1/2000\nUSC00440766\n37.2039\n-80.4144\n641.0\nNaN\nNaN\n0.00\n0.0\n0.0\n56.0\n25.0\n33.0\n\n\n1/2/2000\nUSC00440766\n37.2039\n-80.4144\n641.0\nNaN\nNaN\n0.00\n0.0\n0.0\n59.0\n33.0\n36.0\n\n\n1/3/2000\nUSC00440766\n37.2039\n-80.4144\n641.0\nNaN\nNaN\n0.00\n0.0\n0.0\n63.0\n34.0\n41.0\n\n\n1/4/2000\nUSC00440766\n37.2039\n-80.4144\n641.0\nNaN\nNaN\n0.00\n0.0\n0.0\n66.0\n38.0\n58.0\n\n\n1/5/2000\nUSC00440766\n37.2039\n-80.4144\n641.0\nNaN\nNaN\n0.03\n0.0\n0.0\n60.0\n26.0\n26.0\n\n\n\n\n\n\n\nTo make the data machine learning ready, we first identify the missing values, for that\n\ndf.apply(pd.isnull).sum()/df.shape[0]\n\nSTATION      0.765982\nLATITUDE     0.765982\nLONGITUDE    0.765982\nELEVATION    0.765982\nDAPR         1.000000\nMDPR         1.000000\nPRCP         0.765982\nSNOW         0.769959\nSNWD         0.769959\nTMAX         0.766145\nTMIN         0.766253\nTOBS         0.766145\ndtype: float64\n\n\nLet’s only take some of the features that we may need and let’s convert them into columns\n\nbb_weather= df[[\"PRCP\", \"SNOW\", \"TMAX\", \"TMIN\"]].copy()\n\nbb_weather.columns = [\"rain\", \"snow\", \"max_T\", \"min_T\"]\n\nbb_weather.head()\n\n\n\n\n\n\n\n\nrain\nsnow\nmax_T\nmin_T\n\n\nDATE\n\n\n\n\n\n\n\n\n1/1/2000\n0.00\n0.0\n56.0\n25.0\n\n\n1/2/2000\n0.00\n0.0\n59.0\n33.0\n\n\n1/3/2000\n0.00\n0.0\n63.0\n34.0\n\n\n1/4/2000\n0.00\n0.0\n66.0\n38.0\n\n\n1/5/2000\n0.03\n0.0\n60.0\n26.0\n\n\n\n\n\n\n\n\nbb_weather.apply(pd.isnull).sum()/bb_weather.shape[0]\n\nrain     0.765982\nsnow     0.769959\nmax_T    0.766145\nmin_T    0.766253\ndtype: float64\n\n\nLet’s find out how many days there were rain in Blacksburg in the data set\n\nbb_weather[\"rain\"].value_counts()\n\nrain\n0.00    5408\n0.01     327\n0.02     214\n0.03     171\n0.05     129\n        ... \n2.31       1\n3.00       1\n1.37       1\n1.89       1\n4.42       1\nName: count, Length: 206, dtype: int64\n\n\nSince we are only concerned about the rainfall in Blacksburg and not the snowfall, let’s delete snowfall column\n\ndel bb_weather['snow']\n\nNow, let’s fill all the days which has rainfall values missing as 0. Here, we can also delete the concerned rows but here we are using 0 as the replacement.\n\nbb_weather[\"rain\"] = bb_weather[\"rain\"].fillna(0)\n\n\nbb_weather.apply(pd.isnull).sum()\n\nrain         0\nmax_T    28319\nmin_T    28323\ndtype: int64\n\n\nSince the weather of following day is mostly similar to the previous day we use forward fill in this case unlike rainfall\n\nbb_weather = bb_weather.fillna(method=\"ffill\")\n\ndocumentation of this file says if any item has 9999 then this is the missing values, so to identify them we have this\n\nbb_weather.apply(lambda x: (x == 9999).sum())\n\nrain     0\nmax_T    0\nmin_T    0\ndtype: int64\n\n\n\nbb_weather.index = pd.to_datetime(bb_weather.index)\n\nlet’s visualize the rainfall in different years.\n\npl.plot(bb_weather['rain'],color='blue',marker='o')\npl.xlabel('Year')\npl.ylabel('Rainfall(cm)')\n\nText(0, 0.5, 'Rainfall(cm)')\n\n\n\n\n\nNow, let’s create a new weather column based on the condition as rainy and non-rainy days.\n\nbb_weather['weather'] = 'not_rainy'\n\nbb_weather.loc[bb_weather['rain'] &gt; 0, 'weather'] = 'rainy'\n\nprint(bb_weather)\n\n            rain  max_T  min_T    weather\nDATE                                     \n2000-01-01  0.00   56.0   25.0  not_rainy\n2000-01-02  0.00   59.0   33.0  not_rainy\n2000-01-03  0.00   63.0   34.0  not_rainy\n2000-01-04  0.00   66.0   38.0  not_rainy\n2000-01-05  0.03   60.0   26.0      rainy\n...          ...    ...    ...        ...\nNaT         0.00   79.0   50.0  not_rainy\nNaT         0.00   79.0   50.0  not_rainy\nNaT         0.00   79.0   50.0  not_rainy\nNaT         0.00   79.0   50.0  not_rainy\nNaT         0.00   79.0   50.0  not_rainy\n\n[36963 rows x 4 columns]\n\n\nSince, it is difficult to deal with the words like rainy and non-rainy to perform our classification, we use one-hot encoding method to convert them into binary numbers as 1 and 0, where 1 represents rainy days.\n\nweather_encoded = pd.get_dummies(bb_weather['weather'], prefix='weather')\n\nbb_weather_encoded = pd.concat([bb_weather, weather_encoded], axis=1)\n\n\nbb_weather_encoded.drop('weather', axis=1, inplace=True)\n\nprint(bb_weather_encoded)\n\n            rain  max_T  min_T  weather_not_rainy  weather_rainy\nDATE                                                            \n2000-01-01  0.00   56.0   25.0               True          False\n2000-01-02  0.00   59.0   33.0               True          False\n2000-01-03  0.00   63.0   34.0               True          False\n2000-01-04  0.00   66.0   38.0               True          False\n2000-01-05  0.03   60.0   26.0              False           True\n...          ...    ...    ...                ...            ...\nNaT         0.00   79.0   50.0               True          False\nNaT         0.00   79.0   50.0               True          False\nNaT         0.00   79.0   50.0               True          False\nNaT         0.00   79.0   50.0               True          False\nNaT         0.00   79.0   50.0               True          False\n\n[36963 rows x 5 columns]\n\n\nSince we may get some of our values as negative also as the temperature might fall below 0 during winter, we convert our temperature in Farenheit to Kelvin using relation\n\\((T-32)*5/9 + 273.15\\)\n\nbb_weather_encoded['min_T(Kelvin)'] = (bb_weather_encoded['min_T'] - 32) * 5/9 + 273.15\nbb_weather_encoded['max_T(Kelvin)'] = (bb_weather_encoded['max_T'] - 32) * 5/9 + 273.15\n\n\nbb_weather_encoded.tail()\n\n\n\n\n\n\n\n\nrain\nmax_T\nmin_T\nweather_not_rainy\nweather_rainy\nmin_T(Kelvin)\nmax_T(Kelvin)\n\n\nDATE\n\n\n\n\n\n\n\n\n\n\n\nNaT\n0.0\n79.0\n50.0\nTrue\nFalse\n283.15\n299.261111\n\n\nNaT\n0.0\n79.0\n50.0\nTrue\nFalse\n283.15\n299.261111\n\n\nNaT\n0.0\n79.0\n50.0\nTrue\nFalse\n283.15\n299.261111\n\n\nNaT\n0.0\n79.0\n50.0\nTrue\nFalse\n283.15\n299.261111\n\n\nNaT\n0.0\n79.0\n50.0\nTrue\nFalse\n283.15\n299.261111\n\n\n\n\n\n\n\nFor simplified operation, we also need to convert the rain column into binary not only the weather column.\n\nrainfall = pd.DataFrame(bb_weather_encoded)\n\nrainfall['rain'] = (rainfall['rain'] &gt; 0).astype(int)\n\nTo make maximum temperature and the minimum temperatures as our main features, let’s use chi-squared test and select K best features.\n\nX = rainfall[['max_T(Kelvin)','min_T(Kelvin)']]\ny = rainfall['rain']\n\nk_best = SelectKBest(score_func=chi2, k=2)\n\nX_new = k_best.fit_transform(X, y)\n\nselected_feature_indices = k_best.get_support(indices=True)\n\nselected_features = X.columns[selected_feature_indices]\n\nprint(\"Selected features:\", selected_features)\n\nSelected features: Index(['max_T(Kelvin)', 'min_T(Kelvin)'], dtype='object')\n\n\nFor the classification of our data, we use the random forest classifier. By using this we find the classification report in the form of confusion matrix visualization.\nFor that let’s separate our data in terms of train and test part.\n\nstart_date_train = '2000-01-01'\nend_date_train = '2017-12-31'\nstart_date_test = '2018-01-01'\nend_date_test = '2023-10-05'\n\nwe then create the mask fo the data ranges and extract the data for train and test and finally apply the random forest classifier.\n\nmask_train = (X.index &gt;= start_date_train) & (X.index &lt;= end_date_train)\nmask_test = (X.index &gt;= start_date_test) & (X.index &lt;= end_date_test)\n\n\nX_train, y_train = X[mask_train], y[mask_train]\nX_test, y_test = X[mask_test], y[mask_test]\n\n\nrf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n\n\nrf_classifier.fit(X_train, y_train)\n\n\ny_pred = rf_classifier.predict(X_test)\n\nNext is finding the classification report and the visualization in confusion matrix.\n\nreport = classification_report(y_test, y_pred)\nprint('Classification Report:\\n', report)\n\n\nconfusion = confusion_matrix(y_test, y_pred)\nprint('Confusion Matrix:\\n', confusion)\n\n\npl.figure(figsize=(8, 6))\nsns.heatmap(confusion, annot=True, fmt='d', cmap='Oranges', xticklabels=np.unique(y), yticklabels=np.unique(y))\npl.xlabel('Predicted')\npl.ylabel('Actual')\npl.title('Confusion Matrix')\npl.show()\n\nClassification Report:\n               precision    recall  f1-score   support\n\n           0       0.70      0.78      0.74      1299\n           1       0.57      0.47      0.52       807\n\n    accuracy                           0.66      2106\n   macro avg       0.64      0.63      0.63      2106\nweighted avg       0.65      0.66      0.66      2106\n\nConfusion Matrix:\n [[1017  282]\n [ 428  379]]\n\n\n\n\n\nNow to see if we can improve our results we perform feature importance.\n\nfeature_importance = rf_classifier.feature_importances_\n\n\nfeature_names = ['max_T(Kelvin)','min_T(Kelvin)']\nimportance_df = pd.DataFrame({'Feature': feature_names, 'Importance': feature_importance})\n\n\nimportance_df = importance_df.sort_values(by='Importance', ascending=False)\n\n\npl.figure(figsize=(10, 6))\npl.bar(importance_df['Feature'], importance_df['Importance'])\npl.xticks(rotation=45)\npl.title('Feature Importance for Snowfall Classification')\npl.show()\n\n\n\n\nThen we define the hyper parameters and their possible values:\nHere, we define the no. of trees in the forest(random forest classifier forest not the forest in nature), its max depth and we define the minimum samples to split internal node.\n\nparam_grid = {\n    'n_estimators': [100, 200, 300],\n    \n    'max_depth': [None, 10, 20],\n    \n    'min_samples_split': [2, 5, 10],\n    \n    'min_samples_leaf': [1, 2, 4],\n    }\n\n\nrandom_search = RandomizedSearchCV(rf_classifier, param_grid, n_iter=5, cv=5, scoring='accuracy', random_state=42)\n\n\nrandom_search.fit(X, y)\n\n\nbest_params = random_search.best_params_\nbest_model = random_search.best_estimator_\n\nprint(\"Best Hyperparameters:\")\nprint(best_params)\n\n\nresults = pd.DataFrame(random_search.cv_results_)\n\nBest Hyperparameters:\n{'n_estimators': 100, 'min_samples_split': 2, 'min_samples_leaf': 4, 'max_depth': None}\n\n\nAnd for the pivot table and confusion matrix visualization\n\npivot_table = results.pivot_table(index=['param_n_estimators', 'param_max_depth'],\n                                  columns='param_min_samples_split',\n                                  values='mean_test_score')\n\n\npl.figure(figsize=(10, 6))\nsns.heatmap(pivot_table, annot=True, fmt=\".3f\", cmap=\"YlGnBu\")\npl.title(\"Grid Search Results for Hyperparameters\")\npl.xlabel(\"min_samples_split\")\npl.ylabel(\"n_estimators / max_depth\")\npl.show()\n\n\n\n\nTo identify, how accurate is the result that we obtained\n\nfinal_model = RandomForestClassifier(\n    n_estimators=100,\n    max_depth=10,\n    min_samples_split=10,\n    min_samples_leaf=2,\n    random_state=42\n)\n\n\nfinal_model.fit(X, y)\n\ny_pred = final_model.predict(X)\n\n\nfinal_accuracy = accuracy_score(y, y_pred)\n\nprint(f\"Final Model Accuracy: {final_accuracy:.2f}\")\n\nFinal Model Accuracy: 0.94\n\n\nAnd hence following the same steps that we did earlier, we get new classification results.\n\nreport1 = classification_report(y, y_pred)\nprint('Classification Report:\\n', report1)\n\nClassification Report:\n               precision    recall  f1-score   support\n\n           0       0.95      0.98      0.97     33721\n           1       0.72      0.50      0.59      3242\n\n    accuracy                           0.94     36963\n   macro avg       0.84      0.74      0.78     36963\nweighted avg       0.93      0.94      0.93     36963\n\n\n\n\nconfusion = confusion_matrix(y, y_pred)\n\n# Create a heatmap of the confusion matrix\npl.figure(figsize=(8, 6))\nsns.heatmap(confusion, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False, xticklabels=True, yticklabels=True)\npl.xlabel('Predicted')\npl.ylabel('Actual')\npl.title('Confusion Matrix')\npl.show()\n\n\n\n\nNow to examine the Random Forest Model’s performance, we first make ROC and Precision-Recall curves.\nIt also help us identify the trade offs involved in different threshold settings.\n\ny_prob = best_model.predict_proba(X_test)[:, 1]\n\nfpr, tpr, thresholds_roc = roc_curve(y_test, y_prob)\n\nauc_roc = roc_auc_score(y_test, y_prob)\n\nprecision, recall, thresholds_pr = precision_recall_curve(y_test, y_prob)\n\nAnd to visualize:\n\npl.figure(figsize=(12, 6))\n\n# Plotting ROC curve:\npl.subplot(1, 2, 1)\npl.plot(fpr, tpr, label=f'ROC Curve (AUC = {auc_roc:.2f})')\npl.plot([0, 1], [0, 1], 'k--', label='Random')\npl.xlabel('False Positive Rate')\npl.ylabel('True Positive Rate')\npl.title('ROC Curve')\npl.legend()\n\n# Plotting PR curve:\npl.subplot(1, 2, 2)\npl.plot(recall, precision, label='Precision-Recall Curve')\npl.xlabel('Recall')\npl.ylabel('Precision')\npl.title('Precision-Recall Curve')\npl.legend()\n\n# Adjusting the layout to prevent overlap of subplots\npl.tight_layout()\n\n\n\n\nAnd Finally for the T-test results and the accuracy scores:\n\nIn the future, if we want to know the accuracy of any dataframe directly we can use the following code to directly give the accuracy score once we upload a file which has been cleaned to make ML ready.\n\n\"\n\n# Accuracy score Prediction\n\n#Load the trained ML model\n\nmodel = joblib.load(\"filename\") \\# Replace with the path to your model file\n\ndef predict_snowfall(min_temp, max_temp): try:\n\n\\# Create a DataFrame with the input data\n\ninput_data = pd.DataFrame({'min_T(Kelvin)': \\[min_temp\\], 'max_T(Kelvin)': \\[max_temp\\]})\n# Make predictions\nprediction = model.predict(input_data)\n\nreturn prediction[0]\nexcept Exception as e: return str(e)\n\nif **name** == '**main**': print(\"Temperature to rainfall Prediction\")\n\nmin_temp = float(input(\"Enter Minimum Temperature (Kelvin):\"))\n\nmax_temp = float(input(\"Enter Maximum Temperature (Kelvin):\"))\nresult = predict_rainfall(min_temp, max_temp) print(f”rainfall Prediction: {result}“) ```\n”"
  },
  {
    "objectID": "posts/Regression/index.html",
    "href": "posts/Regression/index.html",
    "title": "Regression",
    "section": "",
    "text": "Importing the required libraries\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as pl\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.metrics import mean_absolute_error, median_absolute_error, explained_variance_score\n\nImporting the dataset\n\ndata=pd.read_csv('Small-Sq-Dev-7-27-2023-2.dat')\ndata.head()\n\n\n\n\n\n\n\n\nTemp (K)\nB Field (T)\nSR830 1 X (V)\nSR830 1 Y (V)\nSR830 2 X (V)\nSR830 2 Y (V)\nP124A (V)\n\n\n\n\n0\n4.224\n-0.170673\n0.0\n0.0\n0.0\n0.0\n0.000091\n\n\n1\n4.225\n-0.170633\n0.0\n0.0\n0.0\n0.0\n0.000091\n\n\n2\n4.224\n-0.170581\n0.0\n0.0\n0.0\n0.0\n0.000091\n\n\n3\n4.225\n-0.170527\n0.0\n0.0\n0.0\n0.0\n0.000091\n\n\n4\n4.225\n-0.170471\n0.0\n0.0\n0.0\n0.0\n0.000091\n\n\n\n\n\n\n\nto identify the missing values\n\ndata.apply(pd.isnull).sum()/data.shape[0]\n\nTemp (K)         0.0\nB Field (T)      0.0\nSR830 1 X (V)    0.0\nSR830 1 Y (V)    0.0\nSR830 2 X (V)    0.0\nSR830 2 Y (V)    0.0\nP124A (V)        0.0\ndtype: float64\n\n\nto obtain the required features and converting them into columns\n\nnew_data= data[[\"B Field (T)\", \"P124A (V)\"]].copy()\n\nnew_data.columns = [\"B_field\", \"Voltage\"]\nnew_data.tail()\n\n\n\n\n\n\n\n\nB_field\nVoltage\n\n\n\n\n5021\n0.175474\n0.000092\n\n\n5022\n0.175544\n0.000092\n\n\n5023\n0.175622\n0.000092\n\n\n5024\n0.175689\n0.000092\n\n\n5025\n0.175751\n0.000092\n\n\n\n\n\n\n\nUsing the relation:\n\\(Resistance(R) = Voltage(V)/ Current(I), from Ohm's law\\)\n\nnew_data['resistance'] = new_data['Voltage'] / 100e-9\nnew_data.head()\n\n\n\n\n\n\n\n\nB_field\nVoltage\nresistance\n\n\n\n\n0\n-0.170673\n0.000091\n913.683\n\n\n1\n-0.170633\n0.000091\n913.937\n\n\n2\n-0.170581\n0.000091\n914.237\n\n\n3\n-0.170527\n0.000091\n914.479\n\n\n4\n-0.170471\n0.000091\n914.669\n\n\n\n\n\n\n\nVisualizing the data:\n\npl.scatter(new_data['B_field'],new_data['resistance'])\npl.xlabel('B Field (T)')\npl.ylabel('Resistance(Ohms)')\npl.title('MAgnetoresistance plot')\n\nText(0.5, 1.0, 'MAgnetoresistance plot')\n\n\n\n\n\nFirst we check if the linear regression works here using the linear relation, if not then we explore other options:\n\n# Split the data into training and test sets\nX_train, X_test, y_train, y_test = train_test_split(new_data[['B_field']], new_data['resistance'], test_size=0.2, random_state=42)\n\n# Linear regression model\nlinear_model = LinearRegression()\nlinear_model.fit(X_train, y_train)\n\n# Predictions\ny_pred = linear_model.predict(X_test)\n\n# Model evaluation\nmse = mean_squared_error(y_test, y_pred)\nr2 = r2_score(y_test, y_pred)\n\nprint(\"Linear Regression:\")\nprint(f\"Mean Squared Error: {mse}\")\nprint(f\"R-squared: {r2}\")\n\n# Visualization\npl.scatter(X_test, y_test, label='Actual')\npl.plot(X_test, y_pred, color='red', label='Predicted')\npl.xlabel('MagneticField')\npl.ylabel('Resistance')\npl.legend()\npl.show()\n\nLinear Regression:\nMean Squared Error: 26034.309179215787\nR-squared: 0.0001171342965614608\n\n\n\n\n\nClearly, we can see that first degree linear regression doesn’t work here, so we try to explore the polynomial regression if it works but for that also we are unknown about the degree of our polynomial so we first identify the best fitting model:\n\nbest_degree = None\nbest_model = None\nbest_mse = float('inf')\nbest_r2 = -float('inf')\n\n# Trying diff polynomial degrees and selecting the best model\nfor degree in range(1, 6):\n  \n    polyreg = make_pipeline(PolynomialFeatures(degree), LinearRegression())\n    polyreg.fit(X_train, y_train)\n\n    # Predictions\n    y_pred_poly = polyreg.predict(X_test)\n\n    # Model evaluation\n    mse_poly = mean_squared_error(y_test, y_pred_poly)\n    r2_poly = r2_score(y_test, y_pred_poly)\n\n    print(f\"Polynomial Regression (Degree {degree}):\")\n    print(f\"Mean Squared Error: {mse_poly}\")\n    print(f\"R-squared: {r2_poly}\")\n\n    if mse_poly &lt; best_mse:\n        best_degree = degree\n        best_model = polyreg\n        best_mse = mse_poly\n        best_r2 = r2_poly\n\nprint(f\"Best Polynomial Model (Degree {best_degree}):\")\nprint(f\"Mean Squared Error: {best_mse}\")\nprint(f\"R-squared: {best_r2}\")\n\nPolynomial Regression (Degree 1):\nMean Squared Error: 26034.309179215787\nR-squared: 0.0001171342965614608\nPolynomial Regression (Degree 2):\nMean Squared Error: 1279.9466108684755\nR-squared: 0.9508419187767698\nPolynomial Regression (Degree 3):\nMean Squared Error: 1280.8042805805371\nR-squared: 0.9508089788111415\nPolynomial Regression (Degree 4):\nMean Squared Error: 244.7914936730683\nR-squared: 0.990598451508402\nPolynomial Regression (Degree 5):\nMean Squared Error: 244.71472498485105\nR-squared: 0.9906013999137329\nBest Polynomial Model (Degree 5):\nMean Squared Error: 244.71472498485105\nR-squared: 0.9906013999137329\n\n\nLet’s try to visualize this:\n\n# Initializing lists:\nmse_values = []\nr2_values = []\n\ndegrees = list(range(1, 6))\n\n# Iterating:\nfor degree in degrees:\n    polyreg = make_pipeline(PolynomialFeatures(degree), LinearRegression())\n    polyreg.fit(X_train, y_train)\n    \n    # Calculating MSE and R-squared\n    mse_poly = mean_squared_error(y_test, polyreg.predict(X_test))\n    r2_poly = r2_score(y_test, polyreg.predict(X_test))\n    \n    mse_values.append(mse_poly)\n    r2_values.append(r2_poly)\n\n\npl.figure(figsize=(12, 5))\npl.subplot(1, 2, 1)\n\npl.plot(degrees, mse_values, marker='o', linestyle='-')\npl.title('Mean Squared Error (MSE) for Polynomial Regression')\npl.xlabel('Polynomial Degree')\npl.ylabel('MSE')\npl.grid(True)\n\n\npl.subplot(1, 2, 2)\npl.plot(degrees, r2_values, marker='o', linestyle='-')\npl.title('R-squared (R^2) for Polynomial Regression')\npl.xlabel('Polynomial Degree')\npl.ylabel('R^2')\npl.grid(True)\n\npl.tight_layout()\npl.show()\n\n\n\n\nFor the absolute error and variance score:\n\ny_true = new_data['resistance']\n\ny_true_subset = y_true[:min(len(y_true), len(y_pred))]\ny_pred_subset = y_pred[:min(len(y_true), len(y_pred))]\n\n\nmae = mean_absolute_error(y_true_subset, y_pred_subset)\nmed_ae = median_absolute_error(y_true_subset, y_pred_subset)\nexplained_var = explained_variance_score(y_true_subset, y_pred_subset)\n\n# Creating a DFrame for visualization:\nmetric_values = pd.DataFrame({\n    'Metric': ['Mean Absolute Error', 'Median Absolute Error', 'Explained Variance Score'],\n    'Value': [mae, med_ae, explained_var]\n})\n\n\npl.figure(figsize=(10, 6))\nsns.barplot(x='Metric', y='Value', data=metric_values, palette='Set3')\npl.title(\"Model Evaluation Metrics\")\npl.ylabel(\"Metric Value\")\npl.xticks(rotation=45)\npl.show()\n\nC:\\Users\\poude\\AppData\\Local\\Temp\\ipykernel_10500\\584074123.py:19: FutureWarning: \n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n\n  sns.barplot(x='Metric', y='Value', data=metric_values, palette='Set3')\n\n\n\n\n\nFrom the above analysis, it is clear that the polynomial of degree 5 is the best option for us, so we can proceed our further analysis by using this degree:\n\nX = new_data[['B_field']]\ny = new_data['resistance']\n\n\ndegree = 5\npolyreg = PolynomialFeatures(degree)\nX_poly = polyreg.fit_transform(X)\n\nmodel = LinearRegression()\nmodel.fit(X_poly, y)\n\ny_pred = model.predict(X_poly)\n\nNow, comparing the actual and predicted values of resistance through a plot.\n\npl.figure(figsize=(10, 6))\nsns.scatterplot(x=X['B_field'], y=y, label='Actual', color='purple')\nsns.scatterplot(x=X['B_field'], y=y_pred, label='Predicted', color='yellow')\npl.xlabel('MagneticField')\npl.ylabel('Resistance')\npl.legend()\npl.title('Actual vs. Predicted Resistance')\npl.show()\n\n\n\n\nAnd the interpretation:\n\ncoefficients = model.coef_\nintercept = model.intercept_\nprint(\"Coefficients:\", coefficients)\nprint(\"Intercept:\", intercept)\n\nCoefficients: [ 0.00000000e+00  9.27243643e+01 -2.94017978e+04  2.21700669e+03\n  4.63232977e+05 -1.43641132e+05]\nIntercept: 1410.5964601492096\n\n\nExamining the model performance:\n\nmse = mean_squared_error(y, y_pred)\nr2 = r2_score(y, y_pred)\nprint(\"Mean Squared Error:\", mse)\nprint(\"R-squared:\", r2)\n\nMean Squared Error: 249.07323073568003\nR-squared: 0.9903722588383516\n\n\nFinally we calculate the residuals, which we also term as smoothing the curve that is removing any kind of background from the data and just finding the peak of resistance amplitude:\n\nresiduals = y - y_pred\n\n# Create a residual plot\npl.figure(figsize=(10, 6))\nsns.scatterplot(x=X['B_field'], y=residuals, color='green')\npl.axhline(y=0, color='red', linestyle='--')\npl.xlabel('MagneticField')\npl.ylabel('Residuals')\npl.title('Residual Plot')\npl.show()\n\n\n\n\nThis shows that if we deduct the parabola from the plot we get this actual relation which makes complete sense and it matches the plot that we get after we use savgol filter of parabola reduction. This proves that our model actually worked."
  }
]