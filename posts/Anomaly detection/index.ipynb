{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Anomaly Detection\"\n",
        "author: \"Rishav Khatiwada\"\n",
        "date: \"2023-11-19\"\n",
        "categories: [news, code, analysis]\n",
        "image: \"image.jpg\"\n",
        "---"
      ],
      "id": "9c086e5d"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Importing the required libraries:\n"
      ],
      "id": "20b2bb2d"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as pl\n",
        "import seaborn as sns\n",
        "from scipy import stats\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn.metrics import silhouette_score, silhouette_samples\n",
        "from sklearn.decomposition import PCA"
      ],
      "id": "1bc9d431",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "importing the dataset:\n"
      ],
      "id": "74be0440"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "data=pd.read_csv('kalimati_tarkari_dataset.csv',index_col='SN')\n",
        "data.head"
      ],
      "id": "fb9044e8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Only selecting the data January 1st 2020 onward which we are interested in:\n"
      ],
      "id": "22f154cd"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "specific_date_1 = pd.to_datetime('2020-01-01') \n",
        "data['Date'] = pd.to_datetime(data['Date'])\n",
        "specific_date = pd.to_datetime(specific_date_1)\n",
        "data = data[data['Date'] > specific_date]\n",
        "data"
      ],
      "id": "5906bf96",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Saving the commodity and date columns in its original form so that it can be used later as we will be converting them using label encoder.\n"
      ],
      "id": "b1c42d7f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "Commodity = data['Commodity'].tolist()\n",
        "Date = data['Date'].tolist()"
      ],
      "id": "892417f6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "pl.scatter(data.iloc[:,3],data.iloc[:,4])"
      ],
      "id": "aff4300e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Identifying the missing values:\n"
      ],
      "id": "d800d3f0"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "data.apply(pd.isnull).sum()/data.shape[0]"
      ],
      "id": "43097b48",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Describing the data to know the details of our features:\n"
      ],
      "id": "f101a83e"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "data.describe()"
      ],
      "id": "550560c7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "sns.distplot(data[\"Average\"])"
      ],
      "id": "787a49db",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "From this we can see that the data is right skewed.\n",
        "\n",
        "To perform the anomaly detection we can use different models. Here, we will discuss some of the models but eventually we will be using DBSCAN.\n",
        "\n",
        "First, we test the z_score to find anomaly.\n"
      ],
      "id": "c3cd930c"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "price = ['Minimum', 'Maximum', 'Average']\n",
        "\n",
        "z_scores = stats.zscore(data[price])\n",
        "\n",
        "threshold = 2\n",
        "\n",
        "anamoly1 = data[(z_scores > threshold).any(axis=1)]\n",
        "print(anamoly1)"
      ],
      "id": "6a086fb4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Second, we use interquartile range(IQR) for anomaly detection.\n"
      ],
      "id": "7cd7d900"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "Q1 = data[price].quantile(0.25)\n",
        "Q3 = data[price].quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "\n",
        "\n",
        "threshold = 1.5\n",
        "\n",
        "\n",
        "anamoly2 = data[((data[price] < (Q1 - threshold * IQR)) | (data[price] > (Q3 + threshold * IQR))).any(axis=1)]\n",
        "\n",
        "print(anamoly2)"
      ],
      "id": "78d2196f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Selecting the numerical features of which we want to detect anomalies:\n"
      ],
      "id": "cdf33b14"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "price = ['Minimum', 'Maximum', 'Average']\n",
        "\n",
        "\n",
        "model = IsolationForest(contamination=0.05)\n",
        "\n",
        "model.fit(data[price])\n",
        "\n",
        "anomaly = model.predict(data[price])\n",
        "anomaly"
      ],
      "id": "dbe88fac",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And performing the element-wise comparison\n"
      ],
      "id": "0e721a09"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "price = np.where(anomaly < 0)\n",
        "price"
      ],
      "id": "4ed0964b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Making our earlier scatter plot more beautiful:\n"
      ],
      "id": "6816c674"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "new_price=data.values\n",
        "pl.scatter(data.iloc[:,3],data.iloc[:,4])\n",
        "pl.scatter(new_price[price,3],new_price[price,4],edgecolor='blue')"
      ],
      "id": "f254f979",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Converting our Date features to a numerical format and converting that timestamp into an integer:\n"
      ],
      "id": "df205cf5"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "data[\"Date\"] = pd.to_datetime(data[\"Date\"]).apply(lambda x: x.timestamp())\n",
        "\n",
        "data[\"Date\"] = data[\"Date\"].astype(int)\n",
        "\n",
        "data.head()"
      ],
      "id": "ab874ece",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "TO maintain the consistency, we choose to use standard scalar to scale our data:\n"
      ],
      "id": "a77af68e"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "data = data.drop(\"Commodity\",axis=1)\n",
        "data = data.drop(\"Unit\",axis=1)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "data = scaler.fit_transform(data)"
      ],
      "id": "e9adc126",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Reducing the dimensions of our data to 2 using PCA:\n"
      ],
      "id": "707931f3"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "num_components = 2\n",
        "pca = PCA(n_components=num_components)\n",
        "\n",
        "data = pca.fit_transform(data)\n",
        "data"
      ],
      "id": "3ebeac4c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Performing the DBSCAN clustering using PCA component 1 and 2 each representing one dimension.\n"
      ],
      "id": "b9fdfc2f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "eps = 0.6\n",
        "min_samples = 10\n",
        "\n",
        "dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
        "dbscan.fit(data)\n",
        "\n",
        "\n",
        "pl.figure(figsize=(10, 6))\n",
        "sns.scatterplot(x=data[:, 0], y=data[:, 1], hue=dbscan.labels_, palette='viridis', legend='full')\n",
        "\n",
        "\n",
        "anomaly_mask = dbscan.labels_ == -1\n",
        "sns.scatterplot(x=data[anomaly_mask, 0], y=data[anomaly_mask, 1], color='red', marker='x', label='Anomalies')\n",
        "\n",
        "pl.title(\"DBSCAN Clustering with Anomalies (PCA-transformed)\")\n",
        "pl.xlabel(\"PCA Component 1\")\n",
        "pl.ylabel(\"PCA Component 2\")\n",
        "pl.legend()\n",
        "pl.show()"
      ],
      "id": "4598ea89",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To see the performance of our Clustering model, we use Silhouette Score:\n"
      ],
      "id": "81510b51"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "silhouette_avg = silhouette_score(data, dbscan.labels_)\n",
        "print(f\"Silhouette Score: {silhouette_avg}\")"
      ],
      "id": "8d8e7aa2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "cluster_labels = dbscan.fit_predict(data)\n",
        "\n",
        "data = pd.DataFrame({'x': data[:, 0], 'y': data[:, 1], 'cluster': cluster_labels, 'Date': Date,\"Commodity\":Commodity})\n",
        "\n",
        "data"
      ],
      "id": "2446745a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "pl.figure(figsize=(10, 6))\n",
        "ax = sns.scatterplot(x=\"x\", y=\"y\", hue=\"cluster\", data=data, palette=\"viridis\", s=100)\n",
        "\n",
        "\n",
        "for x, y, Date, cluster in zip(data['x'], data['y'], data['Date'], data['cluster']):\n",
        "    pl.text(x, y, Date, fontsize=10, alpha=0.8)\n",
        "\n",
        "\n",
        "ax.set(ylim=(-3, 3))\n",
        "pl.xlabel(\"Principal Component 1\", fontsize=15)\n",
        "pl.ylabel(\"Principal Component 2\", fontsize=15)\n",
        "\n",
        "\n",
        "pl.legend(title='Cluster', loc='upper right', labels=[f'Cluster {label}' for label in data['cluster'].unique()])\n",
        "\n",
        "\n",
        "pl.show()"
      ],
      "id": "14f572ea",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It is difficult to visualize individual data with so much compact cluster. So, let's try to do it the other way where we break our clusters and visualize only the one that is important to us.\n",
        "\n",
        "Here, these are the anomaly that we wanted to see.\n"
      ],
      "id": "f897b38a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "data = data[data['cluster'] == -1]\n",
        "\n",
        "ax = sns.scatterplot(x=\"x\", y=\"y\", data=data, color=\"red\", s=100)\n",
        "\n",
        "\n",
        "for x, y, Commodity in zip(data['x'], data['y'], data['Commodity']):\n",
        "    pl.text(x, y, Commodity, fontsize=10, alpha=0.8)\n",
        "\n",
        "\n",
        "ax.set(ylim=(-3, 3))\n",
        "pl.xlabel(\"Principal Component 1\", fontsize=15)\n",
        "pl.ylabel(\"Principal Component 2\", fontsize=15)\n",
        "\n",
        "pl.show()"
      ],
      "id": "62af1ba7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's visualize it more clearly the other way:\n"
      ],
      "id": "9fc1dbc1"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "data = data[data['cluster'] == -1]\n",
        "\n",
        "pl.figure(figsize=(10, 6))\n",
        "sns.countplot(y='Commodity', data=data, color='green')\n",
        "pl.xlabel(\"Count\", fontsize=15)\n",
        "pl.ylabel(\"Commodity\", fontsize=15)\n",
        "pl.title(\"Commodity Counts in Cluster -1\", fontsize=20)\n",
        "pl.show()"
      ],
      "id": "74f3de9b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can see that the Vegetables like Asparagus and Mushroom, spice like Akbare Green Chilli, Fruits like Strawberry which are less consumed in Nepal and are usually more expensive than other vegetables see the anomalies in price. This is simply because people are usually unaware of their actual price as these foods are less consumed in Kathmandu and the whole sellers and retailers take an advantage of this and rise their price citing various reasons like weather, change in fuel price, etc. The suprising commodity that features in this list is Chinese Garlic, which is a popular and most sold spice in Kathmandu. The reason may be the sellers sometime increase the price by creating fake shortage of this product for more profit by saying there has been some problem during import as it is imported from China. This is not very uncommon thing there. So, the concerned authority should really need to pay attention to the sudden increase of the off seasonal and less consumed commodity in addition to the regular ones."
      ],
      "id": "874a27f7"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}