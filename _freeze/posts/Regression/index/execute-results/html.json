{
  "hash": "3c04ca75e708dfa502089acea5373be2",
  "result": {
    "markdown": "---\ntitle: \"Regression\"\nauthor: \"Rishav Khatiwada\"\ndate: \"2023-11-19\"\ncategories: [news, code, analysis]\nimage: \"image.jpg\"\n---\n\nImporting the required libraries\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as pl\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.metrics import mean_absolute_error, median_absolute_error, explained_variance_score\n```\n:::\n\n\nImporting the dataset\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\ndata=pd.read_csv('Small-Sq-Dev-7-27-2023-2.dat')\ndata.head()\n```\n\n::: {.cell-output .cell-output-display execution_count=2}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Temp (K)</th>\n      <th>B Field (T)</th>\n      <th>SR830 1 X (V)</th>\n      <th>SR830 1 Y (V)</th>\n      <th>SR830 2 X (V)</th>\n      <th>SR830 2 Y (V)</th>\n      <th>P124A (V)</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>4.224</td>\n      <td>-0.170673</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000091</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4.225</td>\n      <td>-0.170633</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000091</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>4.224</td>\n      <td>-0.170581</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000091</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4.225</td>\n      <td>-0.170527</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000091</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4.225</td>\n      <td>-0.170471</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000091</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nto identify the missing values\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\ndata.apply(pd.isnull).sum()/data.shape[0]\n```\n\n::: {.cell-output .cell-output-display execution_count=3}\n```\nTemp (K)         0.0\nB Field (T)      0.0\nSR830 1 X (V)    0.0\nSR830 1 Y (V)    0.0\nSR830 2 X (V)    0.0\nSR830 2 Y (V)    0.0\nP124A (V)        0.0\ndtype: float64\n```\n:::\n:::\n\n\nto obtain the required features and converting them into columns\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\nnew_data= data[[\"B Field (T)\", \"P124A (V)\"]].copy()\n\nnew_data.columns = [\"B_field\", \"Voltage\"]\nnew_data.tail()\n```\n\n::: {.cell-output .cell-output-display execution_count=4}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>B_field</th>\n      <th>Voltage</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>5021</th>\n      <td>0.175474</td>\n      <td>0.000092</td>\n    </tr>\n    <tr>\n      <th>5022</th>\n      <td>0.175544</td>\n      <td>0.000092</td>\n    </tr>\n    <tr>\n      <th>5023</th>\n      <td>0.175622</td>\n      <td>0.000092</td>\n    </tr>\n    <tr>\n      <th>5024</th>\n      <td>0.175689</td>\n      <td>0.000092</td>\n    </tr>\n    <tr>\n      <th>5025</th>\n      <td>0.175751</td>\n      <td>0.000092</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nUsing the relation:\n\n$Resistance(R) = Voltage(V)/ Current(I),    from Ohm's law$\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\nnew_data['resistance'] = new_data['Voltage'] / 100e-9\nnew_data.head()\n```\n\n::: {.cell-output .cell-output-display execution_count=5}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>B_field</th>\n      <th>Voltage</th>\n      <th>resistance</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-0.170673</td>\n      <td>0.000091</td>\n      <td>913.683</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-0.170633</td>\n      <td>0.000091</td>\n      <td>913.937</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>-0.170581</td>\n      <td>0.000091</td>\n      <td>914.237</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>-0.170527</td>\n      <td>0.000091</td>\n      <td>914.479</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>-0.170471</td>\n      <td>0.000091</td>\n      <td>914.669</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nVisualizing the data:\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\npl.scatter(new_data['B_field'],new_data['resistance'])\npl.xlabel('B Field (T)')\npl.ylabel('Resistance(Ohms)')\npl.title('MAgnetoresistance plot')\n```\n\n::: {.cell-output .cell-output-display execution_count=6}\n```\nText(0.5, 1.0, 'MAgnetoresistance plot')\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-7-output-2.png){width=601 height=449}\n:::\n:::\n\n\nFirst we check if the linear regression works here using the linear relation, if not then we explore other options:\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\n# Split the data into training and test sets\nX_train, X_test, y_train, y_test = train_test_split(new_data[['B_field']], new_data['resistance'], test_size=0.2, random_state=42)\n\n# Linear regression model\nlinear_model = LinearRegression()\nlinear_model.fit(X_train, y_train)\n\n# Predictions\ny_pred = linear_model.predict(X_test)\n\n# Model evaluation\nmse = mean_squared_error(y_test, y_pred)\nr2 = r2_score(y_test, y_pred)\n\nprint(\"Linear Regression:\")\nprint(f\"Mean Squared Error: {mse}\")\nprint(f\"R-squared: {r2}\")\n\n# Visualization\npl.scatter(X_test, y_test, label='Actual')\npl.plot(X_test, y_pred, color='red', label='Predicted')\npl.xlabel('MagneticField')\npl.ylabel('Resistance')\npl.legend()\npl.show()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLinear Regression:\nMean Squared Error: 26034.309179215787\nR-squared: 0.0001171342965614608\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-8-output-2.png){width=601 height=429}\n:::\n:::\n\n\nClearly, we can see that first degree linear regression doesn't work here, so we try to explore the polynomial regression if it works but for that also we are unknown about the degree of our polynomial so we first identify the best fitting model:\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\nbest_degree = None\nbest_model = None\nbest_mse = float('inf')\nbest_r2 = -float('inf')\n\n# Trying diff polynomial degrees and selecting the best model\nfor degree in range(1, 6):\n  \n    polyreg = make_pipeline(PolynomialFeatures(degree), LinearRegression())\n    polyreg.fit(X_train, y_train)\n\n    # Predictions\n    y_pred_poly = polyreg.predict(X_test)\n\n    # Model evaluation\n    mse_poly = mean_squared_error(y_test, y_pred_poly)\n    r2_poly = r2_score(y_test, y_pred_poly)\n\n    print(f\"Polynomial Regression (Degree {degree}):\")\n    print(f\"Mean Squared Error: {mse_poly}\")\n    print(f\"R-squared: {r2_poly}\")\n\n    if mse_poly < best_mse:\n        best_degree = degree\n        best_model = polyreg\n        best_mse = mse_poly\n        best_r2 = r2_poly\n\nprint(f\"Best Polynomial Model (Degree {best_degree}):\")\nprint(f\"Mean Squared Error: {best_mse}\")\nprint(f\"R-squared: {best_r2}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nPolynomial Regression (Degree 1):\nMean Squared Error: 26034.309179215787\nR-squared: 0.0001171342965614608\nPolynomial Regression (Degree 2):\nMean Squared Error: 1279.9466108684755\nR-squared: 0.9508419187767698\nPolynomial Regression (Degree 3):\nMean Squared Error: 1280.8042805805371\nR-squared: 0.9508089788111415\nPolynomial Regression (Degree 4):\nMean Squared Error: 244.7914936730683\nR-squared: 0.990598451508402\nPolynomial Regression (Degree 5):\nMean Squared Error: 244.71472498485105\nR-squared: 0.9906013999137329\nBest Polynomial Model (Degree 5):\nMean Squared Error: 244.71472498485105\nR-squared: 0.9906013999137329\n```\n:::\n:::\n\n\nLet's try to visualize this:\n\n::: {.cell execution_count=9}\n``` {.python .cell-code}\n# Initializing lists:\nmse_values = []\nr2_values = []\n\ndegrees = list(range(1, 6))\n\n# Iterating:\nfor degree in degrees:\n    polyreg = make_pipeline(PolynomialFeatures(degree), LinearRegression())\n    polyreg.fit(X_train, y_train)\n    \n    # Calculating MSE and R-squared\n    mse_poly = mean_squared_error(y_test, polyreg.predict(X_test))\n    r2_poly = r2_score(y_test, polyreg.predict(X_test))\n    \n    mse_values.append(mse_poly)\n    r2_values.append(r2_poly)\n\n\npl.figure(figsize=(12, 5))\npl.subplot(1, 2, 1)\n\npl.plot(degrees, mse_values, marker='o', linestyle='-')\npl.title('Mean Squared Error (MSE) for Polynomial Regression')\npl.xlabel('Polynomial Degree')\npl.ylabel('MSE')\npl.grid(True)\n\n\npl.subplot(1, 2, 2)\npl.plot(degrees, r2_values, marker='o', linestyle='-')\npl.title('R-squared (R^2) for Polynomial Regression')\npl.xlabel('Polynomial Degree')\npl.ylabel('R^2')\npl.grid(True)\n\npl.tight_layout()\npl.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-10-output-1.png){width=1142 height=470}\n:::\n:::\n\n\nFor the absolute error and variance score:\n\n::: {.cell execution_count=10}\n``` {.python .cell-code}\ny_true = new_data['resistance']\n\ny_true_subset = y_true[:min(len(y_true), len(y_pred))]\ny_pred_subset = y_pred[:min(len(y_true), len(y_pred))]\n\n\nmae = mean_absolute_error(y_true_subset, y_pred_subset)\nmed_ae = median_absolute_error(y_true_subset, y_pred_subset)\nexplained_var = explained_variance_score(y_true_subset, y_pred_subset)\n\n# Creating a DFrame for visualization:\nmetric_values = pd.DataFrame({\n    'Metric': ['Mean Absolute Error', 'Median Absolute Error', 'Explained Variance Score'],\n    'Value': [mae, med_ae, explained_var]\n})\n\n\npl.figure(figsize=(10, 6))\nsns.barplot(x='Metric', y='Value', data=metric_values, palette='Set3')\npl.title(\"Model Evaluation Metrics\")\npl.ylabel(\"Metric Value\")\npl.xticks(rotation=45)\npl.show()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nC:\\Users\\poude\\AppData\\Local\\Temp\\ipykernel_10500\\584074123.py:19: FutureWarning: \n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n\n  sns.barplot(x='Metric', y='Value', data=metric_values, palette='Set3')\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-11-output-2.png){width=816 height=639}\n:::\n:::\n\n\nFrom the above analysis, it is clear that the polynomial of degree 5 is the best option for us, so we can proceed our further analysis by using this degree:\n\n::: {.cell execution_count=11}\n``` {.python .cell-code}\nX = new_data[['B_field']]\ny = new_data['resistance']\n\n\ndegree = 5\npolyreg = PolynomialFeatures(degree)\nX_poly = polyreg.fit_transform(X)\n\nmodel = LinearRegression()\nmodel.fit(X_poly, y)\n\ny_pred = model.predict(X_poly)\n```\n:::\n\n\nNow, comparing the actual and predicted values of resistance through a plot.\n\n::: {.cell execution_count=12}\n``` {.python .cell-code}\npl.figure(figsize=(10, 6))\nsns.scatterplot(x=X['B_field'], y=y, label='Actual', color='purple')\nsns.scatterplot(x=X['B_field'], y=y_pred, label='Predicted', color='yellow')\npl.xlabel('MagneticField')\npl.ylabel('Resistance')\npl.legend()\npl.title('Actual vs. Predicted Resistance')\npl.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-13-output-1.png){width=825 height=523}\n:::\n:::\n\n\nAnd the interpretation:\n\n::: {.cell execution_count=13}\n``` {.python .cell-code}\ncoefficients = model.coef_\nintercept = model.intercept_\nprint(\"Coefficients:\", coefficients)\nprint(\"Intercept:\", intercept)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCoefficients: [ 0.00000000e+00  9.27243643e+01 -2.94017978e+04  2.21700669e+03\n  4.63232977e+05 -1.43641132e+05]\nIntercept: 1410.5964601492096\n```\n:::\n:::\n\n\nExamining the model performance:\n\n::: {.cell execution_count=14}\n``` {.python .cell-code}\nmse = mean_squared_error(y, y_pred)\nr2 = r2_score(y, y_pred)\nprint(\"Mean Squared Error:\", mse)\nprint(\"R-squared:\", r2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nMean Squared Error: 249.07323073568003\nR-squared: 0.9903722588383516\n```\n:::\n:::\n\n\nFinally we calculate the residuals, which we also term as smoothing the curve that is removing any kind of background from the data and just finding the peak of resistance amplitude:\n\n::: {.cell execution_count=15}\n``` {.python .cell-code}\nresiduals = y - y_pred\n\n# Create a residual plot\npl.figure(figsize=(10, 6))\nsns.scatterplot(x=X['B_field'], y=residuals, color='green')\npl.axhline(y=0, color='red', linestyle='--')\npl.xlabel('MagneticField')\npl.ylabel('Residuals')\npl.title('Residual Plot')\npl.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-16-output-1.png){width=819 height=523}\n:::\n:::\n\n\nThis shows that if we deduct the parabola from the plot we get this actual relation which makes complete sense and it matches the plot that we get after we use savgol filter of parabola reduction. This proves that our model actually worked.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}