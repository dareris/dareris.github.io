[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "dareris.github.io",
    "section": "",
    "text": "Anomaly Detection\n\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nNov 19, 2023\n\n\nRishav Khatiwada\n\n\n\n\n\n\n  \n\n\n\n\nClassification\n\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nNov 19, 2023\n\n\nRishav Khatiwada\n\n\n\n\n\n\n  \n\n\n\n\nK-means Clustering\n\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nNov 19, 2023\n\n\nRishav Khatiwada\n\n\n\n\n\n\n  \n\n\n\n\nPost With Code\n\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nNov 19, 2023\n\n\nHarlow Malloc\n\n\n\n\n\n\n  \n\n\n\n\nRegression\n\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nNov 19, 2023\n\n\nRishav Khatiwada\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/Anomaly detection/index.html",
    "href": "posts/Anomaly detection/index.html",
    "title": "Anomaly Detection",
    "section": "",
    "text": "Importing the required libraries:\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as pl\nimport seaborn as sns\nfrom scipy import stats\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cluster import DBSCAN\nfrom sklearn.metrics import silhouette_score, silhouette_samples\nfrom sklearn.decomposition import PCA\n\nimporting the dataset:\n\ndata=pd.read_csv('kalimati_tarkari_dataset.csv',index_col='SN')\ndata.head\n\n&lt;bound method NDFrame.head of                   Commodity        Date Unit  Minimum  Maximum  Average\nSN                                                                     \n0        Tomato Big(Nepali)  2013-06-16   Kg     35.0     40.0     37.5\n1       Tomato Small(Local)  2013-06-16   Kg     26.0     32.0     29.0\n2                Potato Red  2013-06-16   Kg     20.0     21.0     20.5\n3              Potato White  2013-06-16   Kg     15.0     16.0     15.5\n4        Onion Dry (Indian)  2013-06-16   Kg     28.0     30.0     29.0\n...                     ...         ...  ...      ...      ...      ...\n197156    Garlic Dry Nepali  2021-05-13   Kg    100.0    120.0    110.0\n197157     Fish Fresh(Rahu)  2021-05-13   KG    270.0    280.0    275.0\n197158  Fish Fresh(Bachuwa)  2021-05-13   KG    225.0    235.0    230.0\n197159   Fish Fresh(Chhadi)  2021-05-13   KG    220.0    230.0    225.0\n197160  Fish Fresh(Mungari)  2021-05-13   KG    240.0    250.0    245.0\n\n[197161 rows x 6 columns]&gt;\n\n\nOnly selecting the data January 1st 2020 onward which we are interested in:\n\nspecific_date_1 = pd.to_datetime('2020-01-01') \ndata['Date'] = pd.to_datetime(data['Date'])\nspecific_date = pd.to_datetime(specific_date_1)\ndata = data[data['Date'] &gt; specific_date]\ndata\n\n\n\n\n\n\n\n\nCommodity\nDate\nUnit\nMinimum\nMaximum\nAverage\n\n\nSN\n\n\n\n\n\n\n\n\n\n\n157524\nTomato Big(Nepali)\n2020-01-02\nKg\n65.0\n70.0\n67.5\n\n\n157525\nTomato Big(Indian)\n2020-01-02\nKg\n65.0\n70.0\n67.5\n\n\n157526\nTomato Small(Local)\n2020-01-02\nKg\n40.0\n50.0\n45.0\n\n\n157527\nTomato Small(Tunnel)\n2020-01-02\nKg\n40.0\n50.0\n45.0\n\n\n157528\nTomato Small(Indian)\n2020-01-02\nKG\n40.0\n50.0\n45.0\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n197156\nGarlic Dry Nepali\n2021-05-13\nKg\n100.0\n120.0\n110.0\n\n\n197157\nFish Fresh(Rahu)\n2021-05-13\nKG\n270.0\n280.0\n275.0\n\n\n197158\nFish Fresh(Bachuwa)\n2021-05-13\nKG\n225.0\n235.0\n230.0\n\n\n197159\nFish Fresh(Chhadi)\n2021-05-13\nKG\n220.0\n230.0\n225.0\n\n\n197160\nFish Fresh(Mungari)\n2021-05-13\nKG\n240.0\n250.0\n245.0\n\n\n\n\n39637 rows × 6 columns\n\n\n\nSaving the commodity and date columns in its original form so that it can be used later as we will be converting them using label encoder.\n\nCommodity = data['Commodity'].tolist()\nDate = data['Date'].tolist()\n\n\npl.scatter(data.iloc[:,3],data.iloc[:,4])\n\n&lt;matplotlib.collections.PathCollection at 0x224a2581810&gt;\n\n\n\n\n\nIdentifying the missing values:\n\ndata.apply(pd.isnull).sum()/data.shape[0]\n\nCommodity    0.0\nDate         0.0\nUnit         0.0\nMinimum      0.0\nMaximum      0.0\nAverage      0.0\ndtype: float64\n\n\nDescribing the data to know the details of our features:\n\ndata.describe()\n\n\n\n\n\n\n\n\nDate\nMinimum\nMaximum\nAverage\n\n\n\n\ncount\n39637\n39637.000000\n39637.000000\n39637.000000\n\n\nmean\n2020-09-27 03:02:53.020158208\n100.141787\n110.051518\n105.096652\n\n\nmin\n2020-01-02 00:00:00\n6.000000\n8.000000\n7.000000\n\n\n25%\n2020-05-25 00:00:00\n40.000000\n50.000000\n45.000000\n\n\n50%\n2020-10-19 00:00:00\n70.000000\n80.000000\n75.000000\n\n\n75%\n2021-02-02 00:00:00\n120.000000\n130.000000\n125.000000\n\n\nmax\n2021-05-13 00:00:00\n1800.000000\n2000.000000\n1900.000000\n\n\nstd\nNaN\n92.813213\n97.867785\n95.277303\n\n\n\n\n\n\n\n\nsns.distplot(data[\"Average\"])\n\nC:\\Users\\poude\\AppData\\Local\\Temp\\ipykernel_19400\\1442402836.py:1: UserWarning: \n\n`distplot` is a deprecated function and will be removed in seaborn v0.14.0.\n\nPlease adapt your code to use either `displot` (a figure-level function with\nsimilar flexibility) or `histplot` (an axes-level function for histograms).\n\nFor a guide to updating your code to use the new functions, please see\nhttps://gist.github.com/mwaskom/de44147ed2974457ad6372750bbe5751\n\n  sns.distplot(data[\"Average\"])\n\n\n&lt;Axes: xlabel='Average', ylabel='Density'&gt;\n\n\n\n\n\nFrom this we can see that the data is right skewed.\nTo perform the anomaly detection we can use different models. Here, we will discuss some of the models but eventually we will be using DBSCAN.\nFirst, we test the z_score to find anomaly.\n\nprice = ['Minimum', 'Maximum', 'Average']\n\nz_scores = stats.zscore(data[price])\n\nthreshold = 2\n\nanamoly1 = data[(z_scores &gt; threshold).any(axis=1)]\nprint(anamoly1)\n\n               Commodity       Date Unit  Minimum  Maximum  Average\nSN                                                                 \n157571  Mushroom(Button) 2020-01-02   KG    330.0    340.0    335.0\n157598        Chilli Dry 2020-01-02   Kg    390.0    400.0    395.0\n157657  Mushroom(Button) 2020-01-03   KG    340.0    350.0    345.0\n157684        Chilli Dry 2020-01-03   Kg    390.0    400.0    395.0\n157742  Mushroom(Button) 2020-01-04   KG    320.0    330.0    325.0\n...                  ...        ...  ...      ...      ...      ...\n197054        Strawberry 2021-05-12   Kg    400.0    500.0    450.0\n197056        Chilli Dry 2021-05-12   Kg    320.0    330.0    325.0\n197114         Asparagus 2021-05-13   Kg   1000.0   1050.0   1025.0\n197145        Strawberry 2021-05-13   Kg    450.0    500.0    475.0\n197147        Chilli Dry 2021-05-13   Kg    320.0    330.0    325.0\n\n[2208 rows x 6 columns]\n\n\nSecond, we use interquartile range(IQR) for anomaly detection.\n\nQ1 = data[price].quantile(0.25)\nQ3 = data[price].quantile(0.75)\nIQR = Q3 - Q1\n\n\nthreshold = 1.5\n\n\nanamoly2 = data[((data[price] &lt; (Q1 - threshold * IQR)) | (data[price] &gt; (Q3 + threshold * IQR))).any(axis=1)]\n\nprint(anamoly2)\n\n               Commodity       Date Unit  Minimum  Maximum  Average\nSN                                                                 \n157571  Mushroom(Button) 2020-01-02   KG    330.0    340.0    335.0\n157575            Celery 2020-01-02   Kg    270.0    280.0    275.0\n157576          Parseley 2020-01-02   Kg    270.0    280.0    275.0\n157578              Mint 2020-01-02   Kg    270.0    280.0    275.0\n157583           Gundruk 2020-01-02   Kg    280.0    290.0    285.0\n...                  ...        ...  ...      ...      ...      ...\n197130       Pomegranate 2021-05-13   Kg    280.0    300.0    290.0\n197141     Pear(Chinese) 2021-05-13   Kg    250.0    260.0    255.0\n197145        Strawberry 2021-05-13   Kg    450.0    500.0    475.0\n197147        Chilli Dry 2021-05-13   Kg    320.0    330.0    325.0\n197157  Fish Fresh(Rahu) 2021-05-13   KG    270.0    280.0    275.0\n\n[4039 rows x 6 columns]\n\n\nSelecting the numerical features of which we want to detect anomalies:\n\nprice = ['Minimum', 'Maximum', 'Average']\n\n\nmodel = IsolationForest(contamination=0.05)\n\nmodel.fit(data[price])\n\nanomaly = model.predict(data[price])\nanomaly\n\narray([1, 1, 1, ..., 1, 1, 1])\n\n\nAnd performing the element-wise comparison\n\nprice = np.where(anomaly &lt; 0)\nprice\n\n(array([   47,    74,   133, ..., 39590, 39621, 39623], dtype=int64),)\n\n\nMaking our earlier scatter plot more beautiful:\n\nnew_price=data.values\npl.scatter(data.iloc[:,3],data.iloc[:,4])\npl.scatter(new_price[price,3],new_price[price,4],edgecolor='blue')\n\n&lt;matplotlib.collections.PathCollection at 0x224a9883710&gt;\n\n\n\n\n\nConverting our Date features to a numerical format and converting that timestamp into an integer:\n\ndata[\"Date\"] = pd.to_datetime(data[\"Date\"]).apply(lambda x: x.timestamp())\n\ndata[\"Date\"] = data[\"Date\"].astype(int)\n\ndata.head()\n\nC:\\Users\\poude\\AppData\\Local\\Temp\\ipykernel_19400\\2041701408.py:1: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  data[\"Date\"] = pd.to_datetime(data[\"Date\"]).apply(lambda x: x.timestamp())\nC:\\Users\\poude\\AppData\\Local\\Temp\\ipykernel_19400\\2041701408.py:3: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  data[\"Date\"] = data[\"Date\"].astype(int)\n\n\n\n\n\n\n\n\n\nCommodity\nDate\nUnit\nMinimum\nMaximum\nAverage\n\n\nSN\n\n\n\n\n\n\n\n\n\n\n157524\nTomato Big(Nepali)\n1577923200\nKg\n65.0\n70.0\n67.5\n\n\n157525\nTomato Big(Indian)\n1577923200\nKg\n65.0\n70.0\n67.5\n\n\n157526\nTomato Small(Local)\n1577923200\nKg\n40.0\n50.0\n45.0\n\n\n157527\nTomato Small(Tunnel)\n1577923200\nKg\n40.0\n50.0\n45.0\n\n\n157528\nTomato Small(Indian)\n1577923200\nKG\n40.0\n50.0\n45.0\n\n\n\n\n\n\n\nTO maintain the consistency, we choose to use standard scalar to scale our data:\n\ndata = data.drop(\"Commodity\",axis=1)\ndata = data.drop(\"Unit\",axis=1)\n\nscaler = StandardScaler()\ndata = scaler.fit_transform(data)\n\nReducing the dimensions of our data to 2 using PCA:\n\nnum_components = 2\npca = PCA(n_components=num_components)\n\ndata = pca.fit_transform(data)\ndata\n\narray([[-0.79822404,  1.77395273],\n       [-0.79822404,  1.77395273],\n       [-1.20721222,  1.74699569],\n       ...,\n       [ 2.36499445, -1.39243537],\n       [ 2.27428372, -1.39829777],\n       [ 2.63712665, -1.37484816]])\n\n\nPerforming the DBSCAN clustering using PCA component 1 and 2 each representing one dimension.\n\neps = 0.6\nmin_samples = 10\n\ndbscan = DBSCAN(eps=eps, min_samples=min_samples)\ndbscan.fit(data)\n\n\npl.figure(figsize=(10, 6))\nsns.scatterplot(x=data[:, 0], y=data[:, 1], hue=dbscan.labels_, palette='viridis', legend='full')\n\n\nanomaly_mask = dbscan.labels_ == -1\nsns.scatterplot(x=data[anomaly_mask, 0], y=data[anomaly_mask, 1], color='red', marker='x', label='Anomalies')\n\npl.title(\"DBSCAN Clustering with Anomalies (PCA-transformed)\")\npl.xlabel(\"PCA Component 1\")\npl.ylabel(\"PCA Component 2\")\npl.legend()\npl.show()\n\n\n\n\nTo see the performance of our Clustering model, we use Silhouette Score:\n\nsilhouette_avg = silhouette_score(data, dbscan.labels_)\nprint(f\"Silhouette Score: {silhouette_avg}\")\n\nSilhouette Score: 0.8130632448850426\n\n\n\ncluster_labels = dbscan.fit_predict(data)\n\ndata = pd.DataFrame({'x': data[:, 0], 'y': data[:, 1], 'cluster': cluster_labels, 'Date': Date,\"Commodity\":Commodity})\n\ndata\n\n\n\n\n\n\n\n\nx\ny\ncluster\nDate\nCommodity\n\n\n\n\n0\n-0.798224\n1.773953\n0\n2020-01-02\nTomato Big(Nepali)\n\n\n1\n-0.798224\n1.773953\n0\n2020-01-02\nTomato Big(Indian)\n\n\n2\n-1.207212\n1.746996\n0\n2020-01-02\nTomato Small(Local)\n\n\n3\n-1.207212\n1.746996\n0\n2020-01-02\nTomato Small(Tunnel)\n\n\n4\n-1.207212\n1.746996\n0\n2020-01-02\nTomato Small(Indian)\n\n\n...\n...\n...\n...\n...\n...\n\n\n39632\n0.186357\n-1.534286\n0\n2021-05-13\nGarlic Dry Nepali\n\n\n39633\n3.181391\n-1.339674\n0\n2021-05-13\nFish Fresh(Rahu)\n\n\n39634\n2.364994\n-1.392435\n0\n2021-05-13\nFish Fresh(Bachuwa)\n\n\n39635\n2.274284\n-1.398298\n0\n2021-05-13\nFish Fresh(Chhadi)\n\n\n39636\n2.637127\n-1.374848\n0\n2021-05-13\nFish Fresh(Mungari)\n\n\n\n\n39637 rows × 5 columns\n\n\n\n\npl.figure(figsize=(10, 6))\nax = sns.scatterplot(x=\"x\", y=\"y\", hue=\"cluster\", data=data, palette=\"viridis\", s=100)\n\n\nfor x, y, Date, cluster in zip(data['x'], data['y'], data['Date'], data['cluster']):\n    pl.text(x, y, Date, fontsize=10, alpha=0.8)\n\n\nax.set(ylim=(-3, 3))\npl.xlabel(\"Principal Component 1\", fontsize=15)\npl.ylabel(\"Principal Component 2\", fontsize=15)\n\n\npl.legend(title='Cluster', loc='upper right', labels=[f'Cluster {label}' for label in data['cluster'].unique()])\n\n\npl.show()\n\n\n\n\nIt is difficult to visualize individual data with so much compact cluster. So, let’s try to do it the other way where we break our clusters and visualize only the one that is important to us.\nHere, these are the anomaly that we wanted to see.\n\ndata = data[data['cluster'] == -1]\n\nax = sns.scatterplot(x=\"x\", y=\"y\", data=data, color=\"red\", s=100)\n\n\nfor x, y, Commodity in zip(data['x'], data['y'], data['Commodity']):\n    pl.text(x, y, Commodity, fontsize=10, alpha=0.8)\n\n\nax.set(ylim=(-3, 3))\npl.xlabel(\"Principal Component 1\", fontsize=15)\npl.ylabel(\"Principal Component 2\", fontsize=15)\n\npl.show()\n\n\n\n\nLet’s visualize it more clearly the other way:\n\ndata = data[data['cluster'] == -1]\n\npl.figure(figsize=(10, 6))\nsns.countplot(y='Commodity', data=data, color='green')\npl.xlabel(\"Count\", fontsize=15)\npl.ylabel(\"Commodity\", fontsize=15)\npl.title(\"Commodity Counts in Cluster -1\", fontsize=20)\npl.show()\n\n\n\n\nWe can see that the Vegetables like Asparagus and Mushroom, spice like Akbare Green Chilli, Fruits like Strawberry which are less consumed in Nepal and are usually more expensive than other vegetables see the anomalies in price. This is simply because people are usually unaware of their actual price as these foods are less consumed in Kathmandu and the whole sellers and retailers take an advantage of this and rise their price citing various reasons like weather, change in fuel price, etc. The suprising commodity that features in this list is Chinese Garlic, which is a popular and most sold spice in Kathmandu. The reason may be the sellers sometime increase the price by creating fake shortage of this product for more profit by saying there has been some problem during import as it is imported from China. This is not very uncommon thing there. So, the concerned authority should really need to pay attention to the sudden increase of the off seasonal and less consumed commodity in addition to the regular ones."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/Classification/index.html",
    "href": "posts/Classification/index.html",
    "title": "Classification",
    "section": "",
    "text": "First we start with importing some of the required libraries\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as pl\nimport seaborn as sns\nfrom sklearn.feature_selection import SelectKBest, chi2\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import roc_curve, roc_auc_score, precision_recall_curve\nfrom scipy.stats import ttest_ind\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom scipy.stats import randint\n\nImporting the data set\n\ndf = pd.read_csv(\"Blacksburg_weather_dataset.csv\", index_col=\"DATE\")\ndf.head()\n\n\n\n\n\n\n\n\nSTATION\nLATITUDE\nLONGITUDE\nELEVATION\nDAPR\nMDPR\nPRCP\nSNOW\nSNWD\nTMAX\nTMIN\nTOBS\n\n\nDATE\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1/1/2000\nUSC00440766\n37.2039\n-80.4144\n641.0\nNaN\nNaN\n0.00\n0.0\n0.0\n56.0\n25.0\n33.0\n\n\n1/2/2000\nUSC00440766\n37.2039\n-80.4144\n641.0\nNaN\nNaN\n0.00\n0.0\n0.0\n59.0\n33.0\n36.0\n\n\n1/3/2000\nUSC00440766\n37.2039\n-80.4144\n641.0\nNaN\nNaN\n0.00\n0.0\n0.0\n63.0\n34.0\n41.0\n\n\n1/4/2000\nUSC00440766\n37.2039\n-80.4144\n641.0\nNaN\nNaN\n0.00\n0.0\n0.0\n66.0\n38.0\n58.0\n\n\n1/5/2000\nUSC00440766\n37.2039\n-80.4144\n641.0\nNaN\nNaN\n0.03\n0.0\n0.0\n60.0\n26.0\n26.0\n\n\n\n\n\n\n\nTo make the data machine learning ready, we first identify the missing values, for that\n\ndf.apply(pd.isnull).sum()/df.shape[0]\n\nSTATION      0.765982\nLATITUDE     0.765982\nLONGITUDE    0.765982\nELEVATION    0.765982\nDAPR         1.000000\nMDPR         1.000000\nPRCP         0.765982\nSNOW         0.769959\nSNWD         0.769959\nTMAX         0.766145\nTMIN         0.766253\nTOBS         0.766145\ndtype: float64\n\n\nLet’s only take some of the features that we may need and let’s convert them into columns\n\nbb_weather= df[[\"PRCP\", \"SNOW\", \"TMAX\", \"TMIN\"]].copy()\n\nbb_weather.columns = [\"rain\", \"snow\", \"max_T\", \"min_T\"]\n\nbb_weather.head()\n\n\n\n\n\n\n\n\nrain\nsnow\nmax_T\nmin_T\n\n\nDATE\n\n\n\n\n\n\n\n\n1/1/2000\n0.00\n0.0\n56.0\n25.0\n\n\n1/2/2000\n0.00\n0.0\n59.0\n33.0\n\n\n1/3/2000\n0.00\n0.0\n63.0\n34.0\n\n\n1/4/2000\n0.00\n0.0\n66.0\n38.0\n\n\n1/5/2000\n0.03\n0.0\n60.0\n26.0\n\n\n\n\n\n\n\n\nbb_weather.apply(pd.isnull).sum()/bb_weather.shape[0]\n\nrain     0.765982\nsnow     0.769959\nmax_T    0.766145\nmin_T    0.766253\ndtype: float64\n\n\nLet’s find out how many days there were rain in Blacksburg in the data set\n\nbb_weather[\"rain\"].value_counts()\n\nrain\n0.00    5408\n0.01     327\n0.02     214\n0.03     171\n0.05     129\n        ... \n2.31       1\n3.00       1\n1.37       1\n1.89       1\n4.42       1\nName: count, Length: 206, dtype: int64\n\n\nSince we are only concerned about the rainfall in Blacksburg and not the snowfall, let’s delete snowfall column\n\ndel bb_weather['snow']\n\nNow, let’s fill all the days which has rainfall values missing as 0. Here, we can also delete the concerned rows but here we are using 0 as the replacement.\n\nbb_weather[\"rain\"] = bb_weather[\"rain\"].fillna(0)\n\n\nbb_weather.apply(pd.isnull).sum()\n\nrain         0\nmax_T    28319\nmin_T    28323\ndtype: int64\n\n\nSince the weather of following day is mostly similar to the previous day we use forward fill in this case unlike rainfall\n\nbb_weather = bb_weather.fillna(method=\"ffill\")\n\ndocumentation of this file says if any item has 9999 then this is the missing values, so to identify them we have this\n\nbb_weather.apply(lambda x: (x == 9999).sum())\n\nrain     0\nmax_T    0\nmin_T    0\ndtype: int64\n\n\n\nbb_weather.index = pd.to_datetime(bb_weather.index)\n\nlet’s visualize the rainfall in different years.\n\npl.plot(bb_weather['rain'],color='blue',marker='o')\npl.xlabel('Year')\npl.ylabel('Rainfall(cm)')\n\nText(0, 0.5, 'Rainfall(cm)')\n\n\n\n\n\nNow, let’s create a new weather column based on the condition as rainy and non-rainy days.\n\nbb_weather['weather'] = 'not_rainy'\n\nbb_weather.loc[bb_weather['rain'] &gt; 0, 'weather'] = 'rainy'\n\nprint(bb_weather)\n\n            rain  max_T  min_T    weather\nDATE                                     \n2000-01-01  0.00   56.0   25.0  not_rainy\n2000-01-02  0.00   59.0   33.0  not_rainy\n2000-01-03  0.00   63.0   34.0  not_rainy\n2000-01-04  0.00   66.0   38.0  not_rainy\n2000-01-05  0.03   60.0   26.0      rainy\n...          ...    ...    ...        ...\nNaT         0.00   79.0   50.0  not_rainy\nNaT         0.00   79.0   50.0  not_rainy\nNaT         0.00   79.0   50.0  not_rainy\nNaT         0.00   79.0   50.0  not_rainy\nNaT         0.00   79.0   50.0  not_rainy\n\n[36963 rows x 4 columns]\n\n\nSince, it is difficult to deal with the words like rainy and non-rainy to perform our classification, we use one-hot encoding method to convert them into binary numbers as 1 and 0, where 1 represents rainy days.\n\nweather_encoded = pd.get_dummies(bb_weather['weather'], prefix='weather')\n\nbb_weather_encoded = pd.concat([bb_weather, weather_encoded], axis=1)\n\n\nbb_weather_encoded.drop('weather', axis=1, inplace=True)\n\nprint(bb_weather_encoded)\n\n            rain  max_T  min_T  weather_not_rainy  weather_rainy\nDATE                                                            \n2000-01-01  0.00   56.0   25.0               True          False\n2000-01-02  0.00   59.0   33.0               True          False\n2000-01-03  0.00   63.0   34.0               True          False\n2000-01-04  0.00   66.0   38.0               True          False\n2000-01-05  0.03   60.0   26.0              False           True\n...          ...    ...    ...                ...            ...\nNaT         0.00   79.0   50.0               True          False\nNaT         0.00   79.0   50.0               True          False\nNaT         0.00   79.0   50.0               True          False\nNaT         0.00   79.0   50.0               True          False\nNaT         0.00   79.0   50.0               True          False\n\n[36963 rows x 5 columns]\n\n\nSince we may get some of our values as negative also as the temperature might fall below 0 during winter, we convert our temperature in Farenheit to Kelvin using relation\n\\((T-32)*5/9 + 273.15\\)\n\nbb_weather_encoded['min_T(Kelvin)'] = (bb_weather_encoded['min_T'] - 32) * 5/9 + 273.15\nbb_weather_encoded['max_T(Kelvin)'] = (bb_weather_encoded['max_T'] - 32) * 5/9 + 273.15\n\n\nbb_weather_encoded.tail()\n\n\n\n\n\n\n\n\nrain\nmax_T\nmin_T\nweather_not_rainy\nweather_rainy\nmin_T(Kelvin)\nmax_T(Kelvin)\n\n\nDATE\n\n\n\n\n\n\n\n\n\n\n\nNaT\n0.0\n79.0\n50.0\nTrue\nFalse\n283.15\n299.261111\n\n\nNaT\n0.0\n79.0\n50.0\nTrue\nFalse\n283.15\n299.261111\n\n\nNaT\n0.0\n79.0\n50.0\nTrue\nFalse\n283.15\n299.261111\n\n\nNaT\n0.0\n79.0\n50.0\nTrue\nFalse\n283.15\n299.261111\n\n\nNaT\n0.0\n79.0\n50.0\nTrue\nFalse\n283.15\n299.261111\n\n\n\n\n\n\n\nFor simplified operation, we also need to convert the rain column into binary not only the weather column.\n\nrainfall = pd.DataFrame(bb_weather_encoded)\n\nrainfall['rain'] = (rainfall['rain'] &gt; 0).astype(int)\n\nTo make maximum temperature and the minimum temperatures as our main features, let’s use chi-squared test and select K best features.\n\nX = rainfall[['max_T(Kelvin)','min_T(Kelvin)']]\ny = rainfall['rain']\n\nk_best = SelectKBest(score_func=chi2, k=2)\n\nX_new = k_best.fit_transform(X, y)\n\nselected_feature_indices = k_best.get_support(indices=True)\n\nselected_features = X.columns[selected_feature_indices]\n\nprint(\"Selected features:\", selected_features)\n\nSelected features: Index(['max_T(Kelvin)', 'min_T(Kelvin)'], dtype='object')\n\n\nFor the classification of our data, we use the random forest classifier. By using this we find the classification report in the form of confusion matrix visualization.\nFor that let’s separate our data in terms of train and test part.\n\nstart_date_train = '2000-01-01'\nend_date_train = '2017-12-31'\nstart_date_test = '2018-01-01'\nend_date_test = '2023-10-05'\n\nwe then create the mask fo the data ranges and extract the data for train and test and finally apply the random forest classifier.\n\nmask_train = (X.index &gt;= start_date_train) & (X.index &lt;= end_date_train)\nmask_test = (X.index &gt;= start_date_test) & (X.index &lt;= end_date_test)\n\n\nX_train, y_train = X[mask_train], y[mask_train]\nX_test, y_test = X[mask_test], y[mask_test]\n\n\nrf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n\n\nrf_classifier.fit(X_train, y_train)\n\n\ny_pred = rf_classifier.predict(X_test)\n\nNext is finding the classification report and the visualization in confusion matrix.\n\nreport = classification_report(y_test, y_pred)\nprint('Classification Report:\\n', report)\n\n\nconfusion = confusion_matrix(y_test, y_pred)\nprint('Confusion Matrix:\\n', confusion)\n\n\npl.figure(figsize=(8, 6))\nsns.heatmap(confusion, annot=True, fmt='d', cmap='Oranges', xticklabels=np.unique(y), yticklabels=np.unique(y))\npl.xlabel('Predicted')\npl.ylabel('Actual')\npl.title('Confusion Matrix')\npl.show()\n\nClassification Report:\n               precision    recall  f1-score   support\n\n           0       0.70      0.78      0.74      1299\n           1       0.57      0.47      0.52       807\n\n    accuracy                           0.66      2106\n   macro avg       0.64      0.63      0.63      2106\nweighted avg       0.65      0.66      0.66      2106\n\nConfusion Matrix:\n [[1017  282]\n [ 428  379]]\n\n\n\n\n\nNow to see if we can improve our results we perform feature importance.\n\nfeature_importance = rf_classifier.feature_importances_\n\n\nfeature_names = ['max_T(Kelvin)','min_T(Kelvin)']\nimportance_df = pd.DataFrame({'Feature': feature_names, 'Importance': feature_importance})\n\n\nimportance_df = importance_df.sort_values(by='Importance', ascending=False)\n\n\npl.figure(figsize=(10, 6))\npl.bar(importance_df['Feature'], importance_df['Importance'])\npl.xticks(rotation=45)\npl.title('Feature Importance for Snowfall Classification')\npl.show()\n\n\n\n\nThen we define the hyper parameters and their possible values:\nHere, we define the no. of trees in the forest(random forest classifier forest not the forest in nature), its max depth and we define the minimum samples to split internal node.\n\nparam_grid = {\n    'n_estimators': [100, 200, 300],\n    \n    'max_depth': [None, 10, 20],\n    \n    'min_samples_split': [2, 5, 10],\n    \n    'min_samples_leaf': [1, 2, 4],\n    }\n\n\nrandom_search = RandomizedSearchCV(rf_classifier, param_grid, n_iter=5, cv=5, scoring='accuracy', random_state=42)\n\n\nrandom_search.fit(X, y)\n\n\nbest_params = random_search.best_params_\nbest_model = random_search.best_estimator_\n\nprint(\"Best Hyperparameters:\")\nprint(best_params)\n\n\nresults = pd.DataFrame(random_search.cv_results_)\n\nBest Hyperparameters:\n{'n_estimators': 100, 'min_samples_split': 2, 'min_samples_leaf': 4, 'max_depth': None}\n\n\nAnd for the pivot table and confusion matrix visualization\n\npivot_table = results.pivot_table(index=['param_n_estimators', 'param_max_depth'],\n                                  columns='param_min_samples_split',\n                                  values='mean_test_score')\n\n\npl.figure(figsize=(10, 6))\nsns.heatmap(pivot_table, annot=True, fmt=\".3f\", cmap=\"YlGnBu\")\npl.title(\"Grid Search Results for Hyperparameters\")\npl.xlabel(\"min_samples_split\")\npl.ylabel(\"n_estimators / max_depth\")\npl.show()\n\n\n\n\nTo identify, how accurate is the result that we obtained\n\nfinal_model = RandomForestClassifier(\n    n_estimators=100,\n    max_depth=10,\n    min_samples_split=10,\n    min_samples_leaf=2,\n    random_state=42\n)\n\n\nfinal_model.fit(X, y)\n\ny_pred = final_model.predict(X)\n\n\nfinal_accuracy = accuracy_score(y, y_pred)\n\nprint(f\"Final Model Accuracy: {final_accuracy:.2f}\")\n\nFinal Model Accuracy: 0.94\n\n\nAnd hence following the same steps that we did earlier, we get new classification results.\n\nreport1 = classification_report(y, y_pred)\nprint('Classification Report:\\n', report1)\n\nClassification Report:\n               precision    recall  f1-score   support\n\n           0       0.95      0.98      0.97     33721\n           1       0.72      0.50      0.59      3242\n\n    accuracy                           0.94     36963\n   macro avg       0.84      0.74      0.78     36963\nweighted avg       0.93      0.94      0.93     36963\n\n\n\n\nconfusion = confusion_matrix(y, y_pred)\n\n# Create a heatmap of the confusion matrix\npl.figure(figsize=(8, 6))\nsns.heatmap(confusion, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False, xticklabels=True, yticklabels=True)\npl.xlabel('Predicted')\npl.ylabel('Actual')\npl.title('Confusion Matrix')\npl.show()\n\n\n\n\nNow to examine the Random Forest Model’s performance, we first make ROC and Precision-Recall curves.\nIt also help us identify the trade offs involved in different threshold settings.\n\ny_prob = best_model.predict_proba(X_test)[:, 1]\n\nfpr, tpr, thresholds_roc = roc_curve(y_test, y_prob)\n\nauc_roc = roc_auc_score(y_test, y_prob)\n\nprecision, recall, thresholds_pr = precision_recall_curve(y_test, y_prob)\n\nAnd to visualize:\n\npl.figure(figsize=(12, 6))\n\n# Plotting ROC curve:\npl.subplot(1, 2, 1)\npl.plot(fpr, tpr, label=f'ROC Curve (AUC = {auc_roc:.2f})')\npl.plot([0, 1], [0, 1], 'k--', label='Random')\npl.xlabel('False Positive Rate')\npl.ylabel('True Positive Rate')\npl.title('ROC Curve')\npl.legend()\n\n# Plotting PR curve:\npl.subplot(1, 2, 2)\npl.plot(recall, precision, label='Precision-Recall Curve')\npl.xlabel('Recall')\npl.ylabel('Precision')\npl.title('Precision-Recall Curve')\npl.legend()\n\n# Adjusting the layout to prevent overlap of subplots\npl.tight_layout()\n\n\n\n\nAnd Finally for the T-test results and the accuracy scores:\n\nIn the future, if we want to know the accuracy of any dataframe directly we can use the following code to directly give the accuracy score once we upload a file which has been cleaned to make ML ready.\n\n\"\n\n# Accuracy score Prediction\n\n#Load the trained ML model\n\nmodel = joblib.load(\"filename\") \\# Replace with the path to your model file\n\ndef predict_snowfall(min_temp, max_temp): try:\n\n\\# Create a DataFrame with the input data\n\ninput_data = pd.DataFrame({'min_T(Kelvin)': \\[min_temp\\], 'max_T(Kelvin)': \\[max_temp\\]})\n# Make predictions\nprediction = model.predict(input_data)\n\nreturn prediction[0]\nexcept Exception as e: return str(e)\n\nif **name** == '**main**': print(\"Temperature to rainfall Prediction\")\n\nmin_temp = float(input(\"Enter Minimum Temperature (Kelvin):\"))\n\nmax_temp = float(input(\"Enter Maximum Temperature (Kelvin):\"))\nresult = predict_rainfall(min_temp, max_temp) print(f”rainfall Prediction: {result}“) ```\n”"
  },
  {
    "objectID": "posts/Regression/index.html",
    "href": "posts/Regression/index.html",
    "title": "Regression",
    "section": "",
    "text": "Importing the required libraries\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as pl\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.metrics import mean_absolute_error, median_absolute_error, explained_variance_score\n\nImporting the dataset\n\ndata=pd.read_csv('Small-Sq-Dev-7-27-2023-2.dat')\ndata.head()\n\n\n\n\n\n\n\n\nTemp (K)\nB Field (T)\nSR830 1 X (V)\nSR830 1 Y (V)\nSR830 2 X (V)\nSR830 2 Y (V)\nP124A (V)\n\n\n\n\n0\n4.224\n-0.170673\n0.0\n0.0\n0.0\n0.0\n0.000091\n\n\n1\n4.225\n-0.170633\n0.0\n0.0\n0.0\n0.0\n0.000091\n\n\n2\n4.224\n-0.170581\n0.0\n0.0\n0.0\n0.0\n0.000091\n\n\n3\n4.225\n-0.170527\n0.0\n0.0\n0.0\n0.0\n0.000091\n\n\n4\n4.225\n-0.170471\n0.0\n0.0\n0.0\n0.0\n0.000091\n\n\n\n\n\n\n\nto identify the missing values\n\ndata.apply(pd.isnull).sum()/data.shape[0]\n\nTemp (K)         0.0\nB Field (T)      0.0\nSR830 1 X (V)    0.0\nSR830 1 Y (V)    0.0\nSR830 2 X (V)    0.0\nSR830 2 Y (V)    0.0\nP124A (V)        0.0\ndtype: float64\n\n\nto obtain the required features and converting them into columns\n\nnew_data= data[[\"B Field (T)\", \"P124A (V)\"]].copy()\n\nnew_data.columns = [\"B_field\", \"Voltage\"]\nnew_data.tail()\n\n\n\n\n\n\n\n\nB_field\nVoltage\n\n\n\n\n5021\n0.175474\n0.000092\n\n\n5022\n0.175544\n0.000092\n\n\n5023\n0.175622\n0.000092\n\n\n5024\n0.175689\n0.000092\n\n\n5025\n0.175751\n0.000092\n\n\n\n\n\n\n\nUsing the relation:\n\\(Resistance(R) = Voltage(V)/ Current(I), from Ohm's law\\)\n\nnew_data['resistance'] = new_data['Voltage'] / 100e-9\nnew_data.head()\n\n\n\n\n\n\n\n\nB_field\nVoltage\nresistance\n\n\n\n\n0\n-0.170673\n0.000091\n913.683\n\n\n1\n-0.170633\n0.000091\n913.937\n\n\n2\n-0.170581\n0.000091\n914.237\n\n\n3\n-0.170527\n0.000091\n914.479\n\n\n4\n-0.170471\n0.000091\n914.669\n\n\n\n\n\n\n\nVisualizing the data:\n\npl.scatter(new_data['B_field'],new_data['resistance'])\npl.xlabel('B Field (T)')\npl.ylabel('Resistance(Ohms)')\npl.title('MAgnetoresistance plot')\n\nText(0.5, 1.0, 'MAgnetoresistance plot')\n\n\n\n\n\nFirst we check if the linear regression works here using the linear relation, if not then we explore other options:\n\n# Split the data into training and test sets\nX_train, X_test, y_train, y_test = train_test_split(new_data[['B_field']], new_data['resistance'], test_size=0.2, random_state=42)\n\n# Linear regression model\nlinear_model = LinearRegression()\nlinear_model.fit(X_train, y_train)\n\n# Predictions\ny_pred = linear_model.predict(X_test)\n\n# Model evaluation\nmse = mean_squared_error(y_test, y_pred)\nr2 = r2_score(y_test, y_pred)\n\nprint(\"Linear Regression:\")\nprint(f\"Mean Squared Error: {mse}\")\nprint(f\"R-squared: {r2}\")\n\n# Visualization\npl.scatter(X_test, y_test, label='Actual')\npl.plot(X_test, y_pred, color='red', label='Predicted')\npl.xlabel('MagneticField')\npl.ylabel('Resistance')\npl.legend()\npl.show()\n\nLinear Regression:\nMean Squared Error: 26034.309179215787\nR-squared: 0.0001171342965614608\n\n\n\n\n\nClearly, we can see that first degree linear regression doesn’t work here, so we try to explore the polynomial regression if it works but for that also we are unknown about the degree of our polynomial so we first identify the best fitting model:\n\nbest_degree = None\nbest_model = None\nbest_mse = float('inf')\nbest_r2 = -float('inf')\n\n# Trying diff polynomial degrees and selecting the best model\nfor degree in range(1, 6):\n  \n    polyreg = make_pipeline(PolynomialFeatures(degree), LinearRegression())\n    polyreg.fit(X_train, y_train)\n\n    # Predictions\n    y_pred_poly = polyreg.predict(X_test)\n\n    # Model evaluation\n    mse_poly = mean_squared_error(y_test, y_pred_poly)\n    r2_poly = r2_score(y_test, y_pred_poly)\n\n    print(f\"Polynomial Regression (Degree {degree}):\")\n    print(f\"Mean Squared Error: {mse_poly}\")\n    print(f\"R-squared: {r2_poly}\")\n\n    if mse_poly &lt; best_mse:\n        best_degree = degree\n        best_model = polyreg\n        best_mse = mse_poly\n        best_r2 = r2_poly\n\nprint(f\"Best Polynomial Model (Degree {best_degree}):\")\nprint(f\"Mean Squared Error: {best_mse}\")\nprint(f\"R-squared: {best_r2}\")\n\nPolynomial Regression (Degree 1):\nMean Squared Error: 26034.309179215787\nR-squared: 0.0001171342965614608\nPolynomial Regression (Degree 2):\nMean Squared Error: 1279.9466108684755\nR-squared: 0.9508419187767698\nPolynomial Regression (Degree 3):\nMean Squared Error: 1280.8042805805371\nR-squared: 0.9508089788111415\nPolynomial Regression (Degree 4):\nMean Squared Error: 244.7914936730683\nR-squared: 0.990598451508402\nPolynomial Regression (Degree 5):\nMean Squared Error: 244.71472498485105\nR-squared: 0.9906013999137329\nBest Polynomial Model (Degree 5):\nMean Squared Error: 244.71472498485105\nR-squared: 0.9906013999137329\n\n\nLet’s try to visualize this:\n\n# Initializing lists:\nmse_values = []\nr2_values = []\n\ndegrees = list(range(1, 6))\n\n# Iterating:\nfor degree in degrees:\n    polyreg = make_pipeline(PolynomialFeatures(degree), LinearRegression())\n    polyreg.fit(X_train, y_train)\n    \n    # Calculating MSE and R-squared\n    mse_poly = mean_squared_error(y_test, polyreg.predict(X_test))\n    r2_poly = r2_score(y_test, polyreg.predict(X_test))\n    \n    mse_values.append(mse_poly)\n    r2_values.append(r2_poly)\n\n\npl.figure(figsize=(12, 5))\npl.subplot(1, 2, 1)\n\npl.plot(degrees, mse_values, marker='o', linestyle='-')\npl.title('Mean Squared Error (MSE) for Polynomial Regression')\npl.xlabel('Polynomial Degree')\npl.ylabel('MSE')\npl.grid(True)\n\n\npl.subplot(1, 2, 2)\npl.plot(degrees, r2_values, marker='o', linestyle='-')\npl.title('R-squared (R^2) for Polynomial Regression')\npl.xlabel('Polynomial Degree')\npl.ylabel('R^2')\npl.grid(True)\n\npl.tight_layout()\npl.show()\n\n\n\n\nFor the absolute error and variance score:\n\ny_true = new_data['resistance']\n\ny_true_subset = y_true[:min(len(y_true), len(y_pred))]\ny_pred_subset = y_pred[:min(len(y_true), len(y_pred))]\n\n\nmae = mean_absolute_error(y_true_subset, y_pred_subset)\nmed_ae = median_absolute_error(y_true_subset, y_pred_subset)\nexplained_var = explained_variance_score(y_true_subset, y_pred_subset)\n\n# Creating a DFrame for visualization:\nmetric_values = pd.DataFrame({\n    'Metric': ['Mean Absolute Error', 'Median Absolute Error', 'Explained Variance Score'],\n    'Value': [mae, med_ae, explained_var]\n})\n\n\npl.figure(figsize=(10, 6))\nsns.barplot(x='Metric', y='Value', data=metric_values, palette='Set3')\npl.title(\"Model Evaluation Metrics\")\npl.ylabel(\"Metric Value\")\npl.xticks(rotation=45)\npl.show()\n\nC:\\Users\\poude\\AppData\\Local\\Temp\\ipykernel_12104\\584074123.py:19: FutureWarning: \n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n\n  sns.barplot(x='Metric', y='Value', data=metric_values, palette='Set3')\n\n\n\n\n\nFrom the above analysis, it is clear that the polynomial of degree 5 is the best option for us, so we can proceed our further analysis by using this degree:\n\nX = new_data[['B_field']]\ny = new_data['resistance']\n\n\ndegree = 5\npolyreg = PolynomialFeatures(degree)\nX_poly = polyreg.fit_transform(X)\n\nmodel = LinearRegression()\nmodel.fit(X_poly, y)\n\ny_pred = model.predict(X_poly)\n\nNow, comparing the actual and predicted values of resistance through a plot.\n\npl.figure(figsize=(10, 6))\nsns.scatterplot(x=X['B_field'], y=y, label='Actual', color='purple')\nsns.scatterplot(x=X['B_field'], y=y_pred, label='Predicted', color='yellow')\npl.xlabel('MagneticField')\npl.ylabel('Resistance')\npl.legend()\npl.title('Actual vs. Predicted Resistance')\npl.show()\n\n\n\n\nAnd the interpretation:\n\ncoefficients = model.coef_\nintercept = model.intercept_\nprint(\"Coefficients:\", coefficients)\nprint(\"Intercept:\", intercept)\n\nCoefficients: [ 0.00000000e+00  9.27243643e+01 -2.94017978e+04  2.21700669e+03\n  4.63232977e+05 -1.43641132e+05]\nIntercept: 1410.5964601492096\n\n\nExamining the model performance:\n\nmse = mean_squared_error(y, y_pred)\nr2 = r2_score(y, y_pred)\nprint(\"Mean Squared Error:\", mse)\nprint(\"R-squared:\", r2)\n\nMean Squared Error: 249.07323073568003\nR-squared: 0.9903722588383516\n\n\nFinally we calculate the residuals, which we also term as smoothing the curve that is removing any kind of background from the data and just finding the peak of resistance amplitude:\n\nresiduals = y - y_pred\n\n# Create a residual plot\npl.figure(figsize=(10, 6))\nsns.scatterplot(x=X['B_field'], y=residuals, color='green')\npl.axhline(y=0, color='red', linestyle='--')\npl.xlabel('MagneticField')\npl.ylabel('Residuals')\npl.title('Residual Plot')\npl.show()\n\n\n\n\nThis shows that if we deduct the parabola from the plot we get this actual relation which makes complete sense and it matches the plot that we get after we use savgol filter of parabola reduction. This proves that our model actually worked."
  },
  {
    "objectID": "posts/K-means Clustering/index.html",
    "href": "posts/K-means Clustering/index.html",
    "title": "K-means Clustering",
    "section": "",
    "text": "Importing the required libraries\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as pl\nimport seaborn as sns\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\n\nImporting the data set:\n\ndata=pd.read_csv('movie_metadata.csv')\ndata.head()\n\n\n\n\n\n\n\n\ncolor\ndirector_name\nnum_critic_for_reviews\nduration\ndirector_facebook_likes\nactor_3_facebook_likes\nactor_2_name\nactor_1_facebook_likes\ngross\ngenres\n...\nnum_user_for_reviews\nlanguage\ncountry\ncontent_rating\nbudget\ntitle_year\nactor_2_facebook_likes\nimdb_score\naspect_ratio\nmovie_facebook_likes\n\n\n\n\n0\nColor\nJames Cameron\n723.0\n178.0\n0.0\n855.0\nJoel David Moore\n1000.0\n760505847.0\nAction|Adventure|Fantasy|Sci-Fi\n...\n3054.0\nEnglish\nUSA\nPG-13\n237000000.0\n2009.0\n936.0\n7.9\n1.78\n33000\n\n\n1\nColor\nGore Verbinski\n302.0\n169.0\n563.0\n1000.0\nOrlando Bloom\n40000.0\n309404152.0\nAction|Adventure|Fantasy\n...\n1238.0\nEnglish\nUSA\nPG-13\n300000000.0\n2007.0\n5000.0\n7.1\n2.35\n0\n\n\n2\nColor\nSam Mendes\n602.0\n148.0\n0.0\n161.0\nRory Kinnear\n11000.0\n200074175.0\nAction|Adventure|Thriller\n...\n994.0\nEnglish\nUK\nPG-13\n245000000.0\n2015.0\n393.0\n6.8\n2.35\n85000\n\n\n3\nColor\nChristopher Nolan\n813.0\n164.0\n22000.0\n23000.0\nChristian Bale\n27000.0\n448130642.0\nAction|Thriller\n...\n2701.0\nEnglish\nUSA\nPG-13\n250000000.0\n2012.0\n23000.0\n8.5\n2.35\n164000\n\n\n4\nNaN\nDoug Walker\nNaN\nNaN\n131.0\nNaN\nRob Walker\n131.0\nNaN\nDocumentary\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n12.0\n7.1\nNaN\n0\n\n\n\n\n5 rows × 28 columns\n\n\n\nFinding out the total elements, frequency and the most frequent data from our data set:\n\ndata.describe(include='object')\n\n\n\n\n\n\n\n\ncolor\ndirector_name\nactor_2_name\ngenres\nactor_1_name\nmovie_title\nactor_3_name\nplot_keywords\nmovie_imdb_link\nlanguage\ncountry\ncontent_rating\n\n\n\n\ncount\n5024\n4939\n5030\n5043\n5036\n5043\n5020\n4890\n5043\n5029\n5038\n4740\n\n\nunique\n2\n2398\n3032\n914\n2097\n4917\n3521\n4760\n4919\n46\n65\n18\n\n\ntop\nColor\nSteven Spielberg\nMorgan Freeman\nDrama\nRobert De Niro\nBen-Hur\nJohn Heard\nbased on novel\nhttp://www.imdb.com/title/tt0232500/?ref_=fn_t...\nEnglish\nUSA\nR\n\n\nfreq\n4815\n26\n20\n236\n49\n3\n8\n4\n3\n4704\n3807\n2118\n\n\n\n\n\n\n\nSelecting the important features that we need and converting them into the columns:\n\nmovie= data[[\"director_name\",'num_critic_for_reviews',\"actor_1_name\",\"genres\", \"duration\",\"actor_1_facebook_likes\",\"actor_2_facebook_likes\",\"content_rating\",\"actor_3_facebook_likes\",\"gross\",\"budget\"]].copy()\n\n\nmovie.columns = [\"director_name\",'critic_review',\"actor1_name\",\"genre\", \"duration\",\"actor1_fb_likes\",\"actor2_fb_likes\",\"content_rating\",\"actor3_fb_likes\", \"gross\",\"budget\"]\n\nmovie.head()\n\n\n\n\n\n\n\n\ndirector_name\ncritic_review\nactor1_name\ngenre\nduration\nactor1_fb_likes\nactor2_fb_likes\ncontent_rating\nactor3_fb_likes\ngross\nbudget\n\n\n\n\n0\nJames Cameron\n723.0\nCCH Pounder\nAction|Adventure|Fantasy|Sci-Fi\n178.0\n1000.0\n936.0\nPG-13\n855.0\n760505847.0\n237000000.0\n\n\n1\nGore Verbinski\n302.0\nJohnny Depp\nAction|Adventure|Fantasy\n169.0\n40000.0\n5000.0\nPG-13\n1000.0\n309404152.0\n300000000.0\n\n\n2\nSam Mendes\n602.0\nChristoph Waltz\nAction|Adventure|Thriller\n148.0\n11000.0\n393.0\nPG-13\n161.0\n200074175.0\n245000000.0\n\n\n3\nChristopher Nolan\n813.0\nTom Hardy\nAction|Thriller\n164.0\n27000.0\n23000.0\nPG-13\n23000.0\n448130642.0\n250000000.0\n\n\n4\nDoug Walker\nNaN\nDoug Walker\nDocumentary\nNaN\n131.0\n12.0\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n\n\n\nTo prepare the data to be machine learning ready, let’s first identify the missing values:\n\nmovie.apply(pd.isnull).sum()/movie.shape[0]\n\ndirector_name      0.020623\ncritic_review      0.009915\nactor1_name        0.001388\ngenre              0.000000\nduration           0.002974\nactor1_fb_likes    0.001388\nactor2_fb_likes    0.002578\ncontent_rating     0.060083\nactor3_fb_likes    0.004561\ngross              0.175292\nbudget             0.097561\ndtype: float64\n\n\nDropping the missing values meaning deleting the entire row of the ones which contains missing data:\n\nmovie.dropna(axis=0, inplace=True)\nmovie\n\n\n\n\n\n\n\n\ndirector_name\ncritic_review\nactor1_name\ngenre\nduration\nactor1_fb_likes\nactor2_fb_likes\ncontent_rating\nactor3_fb_likes\ngross\nbudget\n\n\n\n\n0\nJames Cameron\n723.0\nCCH Pounder\nAction|Adventure|Fantasy|Sci-Fi\n178.0\n1000.0\n936.0\nPG-13\n855.0\n760505847.0\n237000000.0\n\n\n1\nGore Verbinski\n302.0\nJohnny Depp\nAction|Adventure|Fantasy\n169.0\n40000.0\n5000.0\nPG-13\n1000.0\n309404152.0\n300000000.0\n\n\n2\nSam Mendes\n602.0\nChristoph Waltz\nAction|Adventure|Thriller\n148.0\n11000.0\n393.0\nPG-13\n161.0\n200074175.0\n245000000.0\n\n\n3\nChristopher Nolan\n813.0\nTom Hardy\nAction|Thriller\n164.0\n27000.0\n23000.0\nPG-13\n23000.0\n448130642.0\n250000000.0\n\n\n5\nAndrew Stanton\n462.0\nDaryl Sabara\nAction|Adventure|Sci-Fi\n132.0\n640.0\n632.0\nPG-13\n530.0\n73058679.0\n263700000.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n5033\nShane Carruth\n143.0\nShane Carruth\nDrama|Sci-Fi|Thriller\n77.0\n291.0\n45.0\nPG-13\n8.0\n424760.0\n7000.0\n\n\n5034\nNeill Dela Llana\n35.0\nIan Gamazon\nThriller\n80.0\n0.0\n0.0\nNot Rated\n0.0\n70071.0\n7000.0\n\n\n5035\nRobert Rodriguez\n56.0\nCarlos Gallardo\nAction|Crime|Drama|Romance|Thriller\n81.0\n121.0\n20.0\nR\n6.0\n2040920.0\n7000.0\n\n\n5037\nEdward Burns\n14.0\nKerry Bishé\nComedy|Drama\n95.0\n296.0\n205.0\nNot Rated\n133.0\n4584.0\n9000.0\n\n\n5042\nJon Gunn\n43.0\nJohn August\nDocumentary\n90.0\n86.0\n23.0\nPG\n16.0\n85222.0\n1100.0\n\n\n\n\n3833 rows × 11 columns\n\n\n\nCalculating the total facebook likes by summing the three related columns:\n\nmovie['total_fb_likes'] = movie['actor1_fb_likes'] + movie['actor2_fb_likes'] + movie['actor3_fb_likes']\n\nmovie = movie.drop(['actor1_fb_likes', 'actor2_fb_likes', 'actor3_fb_likes'], axis=1)\nmovie.head()\n\n\n\n\n\n\n\n\ndirector_name\ncritic_review\nactor1_name\ngenre\nduration\ncontent_rating\ngross\nbudget\ntotal_fb_likes\n\n\n\n\n0\nJames Cameron\n723.0\nCCH Pounder\nAction|Adventure|Fantasy|Sci-Fi\n178.0\nPG-13\n760505847.0\n237000000.0\n2791.0\n\n\n1\nGore Verbinski\n302.0\nJohnny Depp\nAction|Adventure|Fantasy\n169.0\nPG-13\n309404152.0\n300000000.0\n46000.0\n\n\n2\nSam Mendes\n602.0\nChristoph Waltz\nAction|Adventure|Thriller\n148.0\nPG-13\n200074175.0\n245000000.0\n11554.0\n\n\n3\nChristopher Nolan\n813.0\nTom Hardy\nAction|Thriller\n164.0\nPG-13\n448130642.0\n250000000.0\n73000.0\n\n\n5\nAndrew Stanton\n462.0\nDaryl Sabara\nAction|Adventure|Sci-Fi\n132.0\nPG-13\n73058679.0\n263700000.0\n1802.0\n\n\n\n\n\n\n\nUsing the label encoder to convert the names into the numerals which our model thrives on:\n\nlabel_encoder = LabelEncoder()\n\n\nmovie['director_name'] = label_encoder.fit_transform(movie['director_name'])\n\nmovie['actor1_name'] = label_encoder.fit_transform(movie['actor1_name'])\n\nmovie['content_rating'] = label_encoder.fit_transform(movie['content_rating'])\n\nmovie['genre'] = label_encoder.fit_transform(movie['genre'])\n\nmovie.tail()\n\n\n\n\n\n\n\n\ndirector_name\ncritic_review\nactor1_name\ngenre\nduration\ncontent_rating\ngross\nbudget\ntotal_fb_likes\n\n\n\n\n5033\n1465\n143.0\n1266\n714\n77.0\n7\n424760.0\n7000.0\n344.0\n\n\n5034\n1164\n35.0\n545\n749\n80.0\n5\n70071.0\n7000.0\n0.0\n\n\n5035\n1379\n56.0\n201\n170\n81.0\n9\n2040920.0\n7000.0\n147.0\n\n\n5037\n428\n14.0\n784\n511\n95.0\n5\n4584.0\n9000.0\n634.0\n\n\n5042\n810\n43.0\n680\n631\n90.0\n6\n85222.0\n1100.0\n125.0\n\n\n\n\n\n\n\nLet’s only take the movies which we assume that were successful:\n\nmovie= movie[movie['gross'] &gt; 3 * movie['budget']]\nmovie\n\n\n\n\n\n\n\n\ndirector_name\ncritic_review\nactor1_name\ngenre\nduration\ncontent_rating\ngross\nbudget\ntotal_fb_likes\n\n\n\n\n0\n637\n723.0\n192\n92\n178.0\n7\n760505847.0\n237000000.0\n2791.0\n\n\n26\n637\n315.0\n845\n705\n194.0\n7\n658672302.0\n200000000.0\n43794.0\n\n\n29\n272\n644.0\n186\n107\n124.0\n7\n652177271.0\n150000000.0\n6000.0\n\n\n100\n1353\n187.0\n1098\n187\n106.0\n7\n144512310.0\n38000000.0\n41000.0\n\n\n186\n478\n502.0\n634\n391\n146.0\n7\n424645577.0\n130000000.0\n48523.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n5027\n629\n64.0\n456\n639\n90.0\n5\n673780.0\n10000.0\n5.0\n\n\n5033\n1465\n143.0\n1266\n714\n77.0\n7\n424760.0\n7000.0\n344.0\n\n\n5034\n1164\n35.0\n545\n749\n80.0\n5\n70071.0\n7000.0\n0.0\n\n\n5035\n1379\n56.0\n201\n170\n81.0\n9\n2040920.0\n7000.0\n147.0\n\n\n5042\n810\n43.0\n680\n631\n90.0\n6\n85222.0\n1100.0\n125.0\n\n\n\n\n678 rows × 9 columns\n\n\n\nTo make our data consistent for each columns so that they can be used to compare to get the required results, we use MInMaxScalar from sklearn library:\n\ncolumns = [\"duration\",\"critic_review\",\"director_name\",\"genre\",\"total_fb_likes\",\"actor1_name\",\"content_rating\"]\n\nscaler = MinMaxScaler()\nmovie[columns] = scaler.fit_transform(movie[columns])\nmovie.head()\n\n\n\n\n\n\n\n\ndirector_name\ncritic_review\nactor1_name\ngenre\nduration\ncontent_rating\ngross\nbudget\ntotal_fb_likes\n\n\n\n\n0\n0.373011\n1.000000\n0.127907\n0.121495\n0.654762\n0.636364\n760505847.0\n237000000.0\n0.004261\n\n\n26\n0.373011\n0.433333\n0.574555\n0.939920\n0.750000\n0.636364\n658672302.0\n200000000.0\n0.066861\n\n\n29\n0.157926\n0.890278\n0.123803\n0.141522\n0.333333\n0.636364\n652177271.0\n150000000.0\n0.009160\n\n\n100\n0.794932\n0.255556\n0.747606\n0.248331\n0.226190\n0.636364\n144512310.0\n38000000.0\n0.062595\n\n\n186\n0.279316\n0.693056\n0.430233\n0.520694\n0.464286\n0.636364\n424645577.0\n130000000.0\n0.074081\n\n\n\n\n\n\n\n\nmovie['genre'].value_counts()\n\ngenre\n0.680908    40\n0.851802    38\n0.640854    36\n0.714286    31\n0.778371    24\n            ..\n0.638184     1\n0.789052     1\n0.871829     1\n0.958611     1\n0.998665     1\nName: count, Length: 220, dtype: int64\n\n\nBefore performing the K-means Clustering, we should examine which of the K values is better for us.\nWe can analyze it with the help of the elbow method:\n\ninertia = []\n\nfor k in range(1, 11):\n    kmeans = KMeans(n_clusters=k, random_state=0)\n    kmeans.fit(movie)\n    inertia.append(kmeans.inertia_)\n\n\npl.figure(figsize=(8, 5))\npl.plot(range(1, 11), inertia, marker='o')\npl.xlabel('Number of Clusters (K)')\npl.ylabel('Inertia')\npl.title('Elbow Method for Optimal K')\npl.grid()\npl.show()\n\nC:\\Users\\poude\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\nC:\\Users\\poude\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\nC:\\Users\\poude\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\nC:\\Users\\poude\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\nC:\\Users\\poude\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\nC:\\Users\\poude\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\nC:\\Users\\poude\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\nC:\\Users\\poude\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\nC:\\Users\\poude\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\nC:\\Users\\poude\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n\n\n\n\n\nTo confirm the above results, let’s also find out the Silhouette Score and the related plots:\n\nsilhouette_scores = []\n\nfor k in range(2, 11):\n    kmeans = KMeans(n_clusters=k, random_state=0)\n    kmeans.fit(movie)  # Use the scaled features\n    silhouette_scores.append(silhouette_score(movie, kmeans.labels_))\n\n\npl.figure(figsize=(8, 5))\npl.plot(range(2, 11), silhouette_scores, marker='o')\npl.xlabel('Number of Clusters (K)')\npl.ylabel('Silhouette Score')\npl.title('Silhouette Score for Optimal K')\npl.grid()\npl.show()\n\nC:\\Users\\poude\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\nC:\\Users\\poude\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\nC:\\Users\\poude\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\nC:\\Users\\poude\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\nC:\\Users\\poude\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\nC:\\Users\\poude\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\nC:\\Users\\poude\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\nC:\\Users\\poude\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\nC:\\Users\\poude\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n\n\n\n\n\nFrom the above two results, we can clearly see that K=2 works best for our model, so let’s use that value and fit K-means:\n\nkmeans = KMeans(n_clusters=2, random_state=0)\n\nkmeans.fit(movie)\n\nC:\\Users\\poude\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n\n\nKMeans(n_clusters=2, random_state=0)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.KMeansKMeans(n_clusters=2, random_state=0)\n\n\n\ncluster_labels = kmeans.labels_\nmovie['kmeans_cluster'] = cluster_labels\nmovie.head()\n\n\n\n\n\n\n\n\ndirector_name\ncritic_review\nactor1_name\ngenre\nduration\ncontent_rating\ngross\nbudget\ntotal_fb_likes\nkmeans_cluster\n\n\n\n\n0\n0.373011\n1.000000\n0.127907\n0.121495\n0.654762\n0.636364\n760505847.0\n237000000.0\n0.004261\n1\n\n\n26\n0.373011\n0.433333\n0.574555\n0.939920\n0.750000\n0.636364\n658672302.0\n200000000.0\n0.066861\n1\n\n\n29\n0.157926\n0.890278\n0.123803\n0.141522\n0.333333\n0.636364\n652177271.0\n150000000.0\n0.009160\n1\n\n\n100\n0.794932\n0.255556\n0.747606\n0.248331\n0.226190\n0.636364\n144512310.0\n38000000.0\n0.062595\n0\n\n\n186\n0.279316\n0.693056\n0.430233\n0.520694\n0.464286\n0.636364\n424645577.0\n130000000.0\n0.074081\n1\n\n\n\n\n\n\n\nTo visualize the clusters, we use Seaborn pair plot function:\n\nsns.pairplot(movie, hue='kmeans_cluster', palette='Dark2')\npl.show()\n\nC:\\Users\\poude\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\seaborn\\axisgrid.py:123: UserWarning: The figure layout has changed to tight\n  self._figure.tight_layout(*args, **kwargs)\n\n\n\n\n\nTo see which features are important to us, we can use the correlation heatmap to compare the features:\n\nnew_movie = movie.copy()\nnew_movie['Cluster'] = cluster_labels\n\ncorrelation_matrix = new_movie.corr()\n\npl.figure(figsize=(12, 8))\nsns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", cbar=True)\npl.title('Correlation Heatmap')\npl.show()\n\n\n\n\nAfter getting the important features from the above plots, we now can reduce out multi-dimentional data into 2 dimensional data using PCA.\n\npca = PCA(n_components=2)\nmovie_pca = pca.fit_transform(movie)\n\nkmeans.fit(movie_pca)\ncluster_labels = kmeans.labels_\n\n\npl.scatter(movie_pca[:, 0], movie_pca[:, 1], c=cluster_labels, cmap='viridis')\npl.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s=300, c='red', label='Cluster Centers')\npl.legend()\npl.title('K-means Clustering')\npl.xlabel('PCA Component 1')\npl.ylabel('PCA Component 2')\npl.show()\n\nC:\\Users\\poude\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n\n\n\n\n\nNow, we can use Random Forest Classifier to determine which features really contribute to the success of the movie. But, before that we need to train our model by splitting our data into training data and testing data.\n\nX = movie.drop(['kmeans_cluster'], axis=1)\ny = movie['kmeans_cluster']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nrf_classifier = RandomForestClassifier()\n\nrf_classifier.fit(X_train, y_train)\n\nfeature_importances = rf_classifier.feature_importances_\n\n\nfeature_importance_movie = pd.DataFrame({'Feature': X.columns, 'Importance': feature_importances})\nfeature_importance_movie = feature_importance_movie.sort_values(by='Importance', ascending=False)\n\n\nprint(feature_importance_movie)\n\n          Feature  Importance\n6           gross    0.720278\n7          budget    0.167511\n3           genre    0.030435\n8  total_fb_likes    0.020072\n1   critic_review    0.015293\n4        duration    0.014967\n5  content_rating    0.011633\n0   director_name    0.010883\n2     actor1_name    0.008928\n\n\nVisualizing this:\n\npl.figure(figsize=(10, 6))\npl.barh(feature_importance_movie['Feature'], feature_importance_movie['Importance'], color='skyblue')\npl.xlabel('Feature Importance')\npl.ylabel('Features')\npl.title('Feature Importance Scores')\npl.gca().invert_yaxis()  # Invert the y-axis for better readability\npl.show()\n\n\n\n\nFrom our analysis, apart from the obvious factors like gross lifetime collection of the movie and budget of the movie, the most important factor that determines the success of the Hollywood movies is the Genre of the movie.\nThis makes complete sense to me because personally me and my friend circles also usually prefer to watch Sci-fi movies over other genre movies. Thus, we can assume that people generally like certain kind of Genre in a movie."
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  }
]