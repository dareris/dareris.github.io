[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "dareris.github.io",
    "section": "",
    "text": "Anomaly Detection\n\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nNov 19, 2023\n\n\nRishav Khatiwada\n\n\n\n\n\n\n  \n\n\n\n\nClassification\n\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nNov 19, 2023\n\n\nRishav Khatiwada\n\n\n\n\n\n\n  \n\n\n\n\nK-means Clustering\n\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nNov 19, 2023\n\n\nRishav Khatiwada\n\n\n\n\n\n\n  \n\n\n\n\nPost With Code\n\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nNov 19, 2023\n\n\nHarlow Malloc\n\n\n\n\n\n\n  \n\n\n\n\nRegression\n\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nNov 19, 2023\n\n\nRishav Khatiwada\n\n\n\n\n\n\n  \n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nNov 16, 2023\n\n\nTristan O’Malley\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/Anomaly detection/index.html",
    "href": "posts/Anomaly detection/index.html",
    "title": "Anomaly Detection",
    "section": "",
    "text": "Importing the required libraries:\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as pl\nimport seaborn as sns\nfrom scipy import stats\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cluster import DBSCAN\nfrom sklearn.metrics import silhouette_score, silhouette_samples\nfrom sklearn.decomposition import PCA\n\nimporting the dataset:\n\ndata=pd.read_csv('kalimati_tarkari_dataset.csv',index_col='SN')\ndata.head\n\n&lt;bound method NDFrame.head of                   Commodity        Date Unit  Minimum  Maximum  Average\nSN                                                                     \n0        Tomato Big(Nepali)  2013-06-16   Kg     35.0     40.0     37.5\n1       Tomato Small(Local)  2013-06-16   Kg     26.0     32.0     29.0\n2                Potato Red  2013-06-16   Kg     20.0     21.0     20.5\n3              Potato White  2013-06-16   Kg     15.0     16.0     15.5\n4        Onion Dry (Indian)  2013-06-16   Kg     28.0     30.0     29.0\n...                     ...         ...  ...      ...      ...      ...\n197156    Garlic Dry Nepali  2021-05-13   Kg    100.0    120.0    110.0\n197157     Fish Fresh(Rahu)  2021-05-13   KG    270.0    280.0    275.0\n197158  Fish Fresh(Bachuwa)  2021-05-13   KG    225.0    235.0    230.0\n197159   Fish Fresh(Chhadi)  2021-05-13   KG    220.0    230.0    225.0\n197160  Fish Fresh(Mungari)  2021-05-13   KG    240.0    250.0    245.0\n\n[197161 rows x 6 columns]&gt;\n\n\nOnly selecting the data January 1st 2020 onward which we are interested in:\n\nspecific_date_1 = pd.to_datetime('2020-01-01') \ndata['Date'] = pd.to_datetime(data['Date'])\nspecific_date = pd.to_datetime(specific_date_1)\ndata = data[data['Date'] &gt; specific_date]\ndata\n\n\n\n\n\n\n\n\nCommodity\nDate\nUnit\nMinimum\nMaximum\nAverage\n\n\nSN\n\n\n\n\n\n\n\n\n\n\n157524\nTomato Big(Nepali)\n2020-01-02\nKg\n65.0\n70.0\n67.5\n\n\n157525\nTomato Big(Indian)\n2020-01-02\nKg\n65.0\n70.0\n67.5\n\n\n157526\nTomato Small(Local)\n2020-01-02\nKg\n40.0\n50.0\n45.0\n\n\n157527\nTomato Small(Tunnel)\n2020-01-02\nKg\n40.0\n50.0\n45.0\n\n\n157528\nTomato Small(Indian)\n2020-01-02\nKG\n40.0\n50.0\n45.0\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n197156\nGarlic Dry Nepali\n2021-05-13\nKg\n100.0\n120.0\n110.0\n\n\n197157\nFish Fresh(Rahu)\n2021-05-13\nKG\n270.0\n280.0\n275.0\n\n\n197158\nFish Fresh(Bachuwa)\n2021-05-13\nKG\n225.0\n235.0\n230.0\n\n\n197159\nFish Fresh(Chhadi)\n2021-05-13\nKG\n220.0\n230.0\n225.0\n\n\n197160\nFish Fresh(Mungari)\n2021-05-13\nKG\n240.0\n250.0\n245.0\n\n\n\n\n39637 rows × 6 columns\n\n\n\nSaving the commodity and date columns in its original form so that it can be used later as we will be converting them using label encoder.\n\nCommodity = data['Commodity'].tolist()\nDate = data['Date'].tolist()\n\n\npl.scatter(data.iloc[:,3],data.iloc[:,4])\n\n&lt;matplotlib.collections.PathCollection at 0x24ff02e4350&gt;\n\n\n\n\n\nIdentifying the missing values:\n\ndata.apply(pd.isnull).sum()/data.shape[0]\n\nCommodity    0.0\nDate         0.0\nUnit         0.0\nMinimum      0.0\nMaximum      0.0\nAverage      0.0\ndtype: float64\n\n\nDescribing the data to know the details of our features:\n\ndata.describe()\n\n\n\n\n\n\n\n\nDate\nMinimum\nMaximum\nAverage\n\n\n\n\ncount\n39637\n39637.000000\n39637.000000\n39637.000000\n\n\nmean\n2020-09-27 03:02:53.020158208\n100.141787\n110.051518\n105.096652\n\n\nmin\n2020-01-02 00:00:00\n6.000000\n8.000000\n7.000000\n\n\n25%\n2020-05-25 00:00:00\n40.000000\n50.000000\n45.000000\n\n\n50%\n2020-10-19 00:00:00\n70.000000\n80.000000\n75.000000\n\n\n75%\n2021-02-02 00:00:00\n120.000000\n130.000000\n125.000000\n\n\nmax\n2021-05-13 00:00:00\n1800.000000\n2000.000000\n1900.000000\n\n\nstd\nNaN\n92.813213\n97.867785\n95.277303\n\n\n\n\n\n\n\n\nsns.distplot(data[\"Average\"])\n\nC:\\Users\\poude\\AppData\\Local\\Temp\\ipykernel_21888\\1442402836.py:1: UserWarning: \n\n`distplot` is a deprecated function and will be removed in seaborn v0.14.0.\n\nPlease adapt your code to use either `displot` (a figure-level function with\nsimilar flexibility) or `histplot` (an axes-level function for histograms).\n\nFor a guide to updating your code to use the new functions, please see\nhttps://gist.github.com/mwaskom/de44147ed2974457ad6372750bbe5751\n\n  sns.distplot(data[\"Average\"])\n\n\n&lt;Axes: xlabel='Average', ylabel='Density'&gt;\n\n\n\n\n\nFrom this we can see that the data is right skewed.\nTo perform the anomaly detection we can use different models. Here, we will discuss some of the models but eventually we will be using DBSCAN.\nFirst, we test the z_score to find anomaly.\n\nprice = ['Minimum', 'Maximum', 'Average']\n\nz_scores = stats.zscore(data[price])\n\nthreshold = 2\n\nanamoly1 = data[(z_scores &gt; threshold).any(axis=1)]\nprint(anamoly1)\n\n               Commodity       Date Unit  Minimum  Maximum  Average\nSN                                                                 \n157571  Mushroom(Button) 2020-01-02   KG    330.0    340.0    335.0\n157598        Chilli Dry 2020-01-02   Kg    390.0    400.0    395.0\n157657  Mushroom(Button) 2020-01-03   KG    340.0    350.0    345.0\n157684        Chilli Dry 2020-01-03   Kg    390.0    400.0    395.0\n157742  Mushroom(Button) 2020-01-04   KG    320.0    330.0    325.0\n...                  ...        ...  ...      ...      ...      ...\n197054        Strawberry 2021-05-12   Kg    400.0    500.0    450.0\n197056        Chilli Dry 2021-05-12   Kg    320.0    330.0    325.0\n197114         Asparagus 2021-05-13   Kg   1000.0   1050.0   1025.0\n197145        Strawberry 2021-05-13   Kg    450.0    500.0    475.0\n197147        Chilli Dry 2021-05-13   Kg    320.0    330.0    325.0\n\n[2208 rows x 6 columns]\n\n\nSecond, we use interquartile range(IQR) for anomaly detection.\n\nQ1 = data[price].quantile(0.25)\nQ3 = data[price].quantile(0.75)\nIQR = Q3 - Q1\n\n\nthreshold = 1.5\n\n\nanamoly2 = data[((data[price] &lt; (Q1 - threshold * IQR)) | (data[price] &gt; (Q3 + threshold * IQR))).any(axis=1)]\n\nprint(anamoly2)\n\n               Commodity       Date Unit  Minimum  Maximum  Average\nSN                                                                 \n157571  Mushroom(Button) 2020-01-02   KG    330.0    340.0    335.0\n157575            Celery 2020-01-02   Kg    270.0    280.0    275.0\n157576          Parseley 2020-01-02   Kg    270.0    280.0    275.0\n157578              Mint 2020-01-02   Kg    270.0    280.0    275.0\n157583           Gundruk 2020-01-02   Kg    280.0    290.0    285.0\n...                  ...        ...  ...      ...      ...      ...\n197130       Pomegranate 2021-05-13   Kg    280.0    300.0    290.0\n197141     Pear(Chinese) 2021-05-13   Kg    250.0    260.0    255.0\n197145        Strawberry 2021-05-13   Kg    450.0    500.0    475.0\n197147        Chilli Dry 2021-05-13   Kg    320.0    330.0    325.0\n197157  Fish Fresh(Rahu) 2021-05-13   KG    270.0    280.0    275.0\n\n[4039 rows x 6 columns]\n\n\nSelecting the numerical features of which we want to detect anomalies:\n\nprice = ['Minimum', 'Maximum', 'Average']\n\n\nmodel = IsolationForest(contamination=0.05)\n\nmodel.fit(data[price])\n\nanomaly = model.predict(data[price])\nanomaly\n\narray([1, 1, 1, ..., 1, 1, 1])\n\n\nAnd performing the element-wise comparison\n\nprice = np.where(anomaly &lt; 0)\nprice\n\n(array([   47,    74,   133, ..., 39590, 39621, 39623], dtype=int64),)\n\n\nMaking our earlier scatter plot more beautiful:\n\nnew_price=data.values\npl.scatter(data.iloc[:,3],data.iloc[:,4])\npl.scatter(new_price[price,3],new_price[price,4],edgecolor='blue')\n\n&lt;matplotlib.collections.PathCollection at 0x24ff749c310&gt;\n\n\n\n\n\nConverting our Date features to a numerical format and converting that timestamp into an integer:\n\ndata[\"Date\"] = pd.to_datetime(data[\"Date\"]).apply(lambda x: x.timestamp())\n\ndata[\"Date\"] = data[\"Date\"].astype(int)\n\ndata.head()\n\nC:\\Users\\poude\\AppData\\Local\\Temp\\ipykernel_21888\\2041701408.py:1: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  data[\"Date\"] = pd.to_datetime(data[\"Date\"]).apply(lambda x: x.timestamp())\nC:\\Users\\poude\\AppData\\Local\\Temp\\ipykernel_21888\\2041701408.py:3: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  data[\"Date\"] = data[\"Date\"].astype(int)\n\n\n\n\n\n\n\n\n\nCommodity\nDate\nUnit\nMinimum\nMaximum\nAverage\n\n\nSN\n\n\n\n\n\n\n\n\n\n\n157524\nTomato Big(Nepali)\n1577923200\nKg\n65.0\n70.0\n67.5\n\n\n157525\nTomato Big(Indian)\n1577923200\nKg\n65.0\n70.0\n67.5\n\n\n157526\nTomato Small(Local)\n1577923200\nKg\n40.0\n50.0\n45.0\n\n\n157527\nTomato Small(Tunnel)\n1577923200\nKg\n40.0\n50.0\n45.0\n\n\n157528\nTomato Small(Indian)\n1577923200\nKG\n40.0\n50.0\n45.0\n\n\n\n\n\n\n\nTO maintain the consistency, we choose to use standard scalar to scale our data:\n\ndata = data.drop(\"Commodity\",axis=1)\ndata = data.drop(\"Unit\",axis=1)\n\nscaler = StandardScaler()\ndata = scaler.fit_transform(data)\n\nReducing the dimensions of our data to 2 using PCA:\n\nnum_components = 2\npca = PCA(n_components=num_components)\n\ndata = pca.fit_transform(data)\ndata\n\narray([[-0.79822404,  1.77395273],\n       [-0.79822404,  1.77395273],\n       [-1.20721222,  1.74699569],\n       ...,\n       [ 2.36499445, -1.39243537],\n       [ 2.27428372, -1.39829777],\n       [ 2.63712665, -1.37484816]])\n\n\nPerforming the DBSCAN clustering using PCA component 1 and 2 each representing one dimension.\n\neps = 0.6\nmin_samples = 10\n\ndbscan = DBSCAN(eps=eps, min_samples=min_samples)\ndbscan.fit(data)\n\n\npl.figure(figsize=(10, 6))\nsns.scatterplot(x=data[:, 0], y=data[:, 1], hue=dbscan.labels_, palette='viridis', legend='full')\n\n\nanomaly_mask = dbscan.labels_ == -1\nsns.scatterplot(x=data[anomaly_mask, 0], y=data[anomaly_mask, 1], color='red', marker='x', label='Anomalies')\n\npl.title(\"DBSCAN Clustering with Anomalies (PCA-transformed)\")\npl.xlabel(\"PCA Component 1\")\npl.ylabel(\"PCA Component 2\")\npl.legend()\npl.show()\n\n\n\n\nTo see the performance of our Clustering model, we use Silhouette Score:\n\nsilhouette_avg = silhouette_score(data, dbscan.labels_)\nprint(f\"Silhouette Score: {silhouette_avg}\")\n\nSilhouette Score: 0.8130632448850409\n\n\n\ncluster_labels = dbscan.fit_predict(data)\n\ndata = pd.DataFrame({'x': data[:, 0], 'y': data[:, 1], 'cluster': cluster_labels, 'Date': Date,\"Commodity\":Commodity})\n\ndata\n\n\n\n\n\n\n\n\nx\ny\ncluster\nDate\nCommodity\n\n\n\n\n0\n-0.798224\n1.773953\n0\n2020-01-02\nTomato Big(Nepali)\n\n\n1\n-0.798224\n1.773953\n0\n2020-01-02\nTomato Big(Indian)\n\n\n2\n-1.207212\n1.746996\n0\n2020-01-02\nTomato Small(Local)\n\n\n3\n-1.207212\n1.746996\n0\n2020-01-02\nTomato Small(Tunnel)\n\n\n4\n-1.207212\n1.746996\n0\n2020-01-02\nTomato Small(Indian)\n\n\n...\n...\n...\n...\n...\n...\n\n\n39632\n0.186357\n-1.534286\n0\n2021-05-13\nGarlic Dry Nepali\n\n\n39633\n3.181391\n-1.339674\n0\n2021-05-13\nFish Fresh(Rahu)\n\n\n39634\n2.364994\n-1.392435\n0\n2021-05-13\nFish Fresh(Bachuwa)\n\n\n39635\n2.274284\n-1.398298\n0\n2021-05-13\nFish Fresh(Chhadi)\n\n\n39636\n2.637127\n-1.374848\n0\n2021-05-13\nFish Fresh(Mungari)\n\n\n\n\n39637 rows × 5 columns\n\n\n\n\npl.figure(figsize=(10, 6))\nax = sns.scatterplot(x=\"x\", y=\"y\", hue=\"cluster\", data=data, palette=\"viridis\", s=100)\n\n\nfor x, y, Date, cluster in zip(data['x'], data['y'], data['Date'], data['cluster']):\n    pl.text(x, y, Date, fontsize=10, alpha=0.8)\n\n\nax.set(ylim=(-3, 3))\npl.xlabel(\"Principal Component 1\", fontsize=15)\npl.ylabel(\"Principal Component 2\", fontsize=15)\n\n\npl.legend(title='Cluster', loc='upper right', labels=[f'Cluster {label}' for label in data['cluster'].unique()])\n\n\npl.show()\n\n\n\n\nIt is difficult to visualize individual data with so much compact cluster. So, let’s try to do it the other way where we break our clusters and visualize only the one that is important to us.\nHere, these are the anomaly that we wanted to see.\n\ndata = data[data['cluster'] == -1]\n\nax = sns.scatterplot(x=\"x\", y=\"y\", data=data, color=\"red\", s=100)\n\n\nfor x, y, Commodity in zip(data['x'], data['y'], data['Commodity']):\n    pl.text(x, y, Commodity, fontsize=10, alpha=0.8)\n\n\nax.set(ylim=(-3, 3))\npl.xlabel(\"Principal Component 1\", fontsize=15)\npl.ylabel(\"Principal Component 2\", fontsize=15)\n\npl.show()\n\n\n\n\nLet’s visualize it more clearly the other way:\n\ndata = data[data['cluster'] == -1]\n\npl.figure(figsize=(10, 6))\nsns.countplot(y='Commodity', data=data, color='green')\npl.xlabel(\"Count\", fontsize=15)\npl.ylabel(\"Commodity\", fontsize=15)\npl.title(\"Commodity Counts in Cluster -1\", fontsize=20)\npl.show()\n\n\n\n\nWe can see that the Vegetables like Asparagus and Mushroom, spice like Akbare Green Chilli, Fruits like Strawberry which are less consumed in Nepal and are usually more expensive than other vegetables see the anomalies in price. This is simply because people are usually unaware of their actual price as these foods are less consumed in Kathmandu and the whole sellers and retailers take an advantage of this and rise their price citing various reasons like weather, change in fuel price, etc. The suprising commodity that features in this list is Chinese Garlic, which is a popular and most sold spice in Kathmandu. The reason may be the sellers sometime increase the price by creating fake shortage of this product for more profit by saying there has been some problem during import as it is imported from China. This is not very uncommon thing there. So, the concerned authority should really need to pay attention to the sudden increase of the off seasonal and less consumed commodity in addition to the regular ones."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  }
]